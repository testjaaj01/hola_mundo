{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is highly recommended to use a powerful **GPU**, you can use it for free uploading this notebook to [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb).\n",
    "<table align=\"center\">\n",
    " <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/ezponda/intro_deep_learning/blob/main/class/CNN/Object_Detection.ipynb\">\n",
    "        <img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
    "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/ezponda/intro_deep_learning/blob/main/class/CNN/Object_Detection.ipynb\">\n",
    "        <img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />View Source on GitHub</a></td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Hub\n",
    "\n",
    "[TensorFlow Hub](https://www.tensorflow.org/hub) is a repository of trained machine learning models. You can install it with: \n",
    "```python\n",
    "!pip install --upgrade tensorflow_hub\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade tensorflow_hub\n",
    "#!pip install opencv-python\n",
    "#!pip install opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from PIL import Image, ImageDraw\n",
    "from IPython.display import Image as IPyImage\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find multiple [models](https://tfhub.dev/tensorflow/collections/object_detection/1) in [Tensorflow-Hub](https://www.tensorflow.org/hub), and [here](https://tfhub.dev/s?module-type=image-object-detection) you can find all the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_MODELS = {\n",
    "    'CenterNet HourGlass104 512x512':\n",
    "    'https://tfhub.dev/tensorflow/centernet/hourglass_512x512/1',\n",
    "    'CenterNet HourGlass104 Keypoints 512x512':\n",
    "    'https://tfhub.dev/tensorflow/centernet/hourglass_512x512_kpts/1',\n",
    "    'CenterNet HourGlass104 1024x1024':\n",
    "    'https://tfhub.dev/tensorflow/centernet/hourglass_1024x1024/1',\n",
    "    'CenterNet HourGlass104 Keypoints 1024x1024':\n",
    "    'https://tfhub.dev/tensorflow/centernet/hourglass_1024x1024_kpts/1',\n",
    "    'CenterNet Resnet50 V1 FPN 512x512':\n",
    "    'https://tfhub.dev/tensorflow/centernet/resnet50v1_fpn_512x512/1',\n",
    "    'CenterNet Resnet50 V1 FPN Keypoints 512x512':\n",
    "    'https://tfhub.dev/tensorflow/centernet/resnet50v1_fpn_512x512_kpts/1',\n",
    "    'CenterNet Resnet101 V1 FPN 512x512':\n",
    "    'https://tfhub.dev/tensorflow/centernet/resnet101v1_fpn_512x512/1',\n",
    "    'CenterNet Resnet50 V2 512x512':\n",
    "    'https://tfhub.dev/tensorflow/centernet/resnet50v2_512x512/1',\n",
    "    'CenterNet Resnet50 V2 Keypoints 512x512':\n",
    "    'https://tfhub.dev/tensorflow/centernet/resnet50v2_512x512_kpts/1',\n",
    "    'EfficientDet D0 512x512':\n",
    "    'https://tfhub.dev/tensorflow/efficientdet/d0/1',\n",
    "    'EfficientDet D1 640x640':\n",
    "    'https://tfhub.dev/tensorflow/efficientdet/d1/1',\n",
    "    'EfficientDet D2 768x768':\n",
    "    'https://tfhub.dev/tensorflow/efficientdet/d2/1',\n",
    "    'EfficientDet D3 896x896':\n",
    "    'https://tfhub.dev/tensorflow/efficientdet/d3/1',\n",
    "    'EfficientDet D4 1024x1024':\n",
    "    'https://tfhub.dev/tensorflow/efficientdet/d4/1',\n",
    "    'EfficientDet D5 1280x1280':\n",
    "    'https://tfhub.dev/tensorflow/efficientdet/d5/1',\n",
    "    'EfficientDet D6 1280x1280':\n",
    "    'https://tfhub.dev/tensorflow/efficientdet/d6/1',\n",
    "    'EfficientDet D7 1536x1536':\n",
    "    'https://tfhub.dev/tensorflow/efficientdet/d7/1',\n",
    "    'SSD MobileNet v2 320x320':\n",
    "    'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2',\n",
    "    'SSD MobileNet V1 FPN 640x640':\n",
    "    'https://tfhub.dev/tensorflow/ssd_mobilenet_v1/fpn_640x640/1',\n",
    "    'SSD MobileNet V2 FPNLite 320x320':\n",
    "    'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_320x320/1',\n",
    "    'SSD MobileNet V2 FPNLite 640x640':\n",
    "    'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_640x640/1',\n",
    "    'SSD ResNet50 V1 FPN 640x640 (RetinaNet50)':\n",
    "    'https://tfhub.dev/tensorflow/retinanet/resnet50_v1_fpn_640x640/1',\n",
    "    'SSD ResNet50 V1 FPN 1024x1024 (RetinaNet50)':\n",
    "    'https://tfhub.dev/tensorflow/retinanet/resnet50_v1_fpn_1024x1024/1',\n",
    "    'SSD ResNet101 V1 FPN 640x640 (RetinaNet101)':\n",
    "    'https://tfhub.dev/tensorflow/retinanet/resnet101_v1_fpn_640x640/1',\n",
    "    'SSD ResNet101 V1 FPN 1024x1024 (RetinaNet101)':\n",
    "    'https://tfhub.dev/tensorflow/retinanet/resnet101_v1_fpn_1024x1024/1',\n",
    "    'SSD ResNet152 V1 FPN 640x640 (RetinaNet152)':\n",
    "    'https://tfhub.dev/tensorflow/retinanet/resnet152_v1_fpn_640x640/1',\n",
    "    'SSD ResNet152 V1 FPN 1024x1024 (RetinaNet152)':\n",
    "    'https://tfhub.dev/tensorflow/retinanet/resnet152_v1_fpn_1024x1024/1',\n",
    "    'Faster R-CNN ResNet50 V1 640x640':\n",
    "    'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_640x640/1',\n",
    "    'Faster R-CNN ResNet50 V1 1024x1024':\n",
    "    'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_1024x1024/1',\n",
    "    'Faster R-CNN ResNet50 V1 800x1333':\n",
    "    'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_800x1333/1',\n",
    "    'Faster R-CNN ResNet101 V1 640x640':\n",
    "    'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_640x640/1',\n",
    "    'Faster R-CNN ResNet101 V1 1024x1024':\n",
    "    'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_1024x1024/1',\n",
    "    'Faster R-CNN ResNet101 V1 800x1333':\n",
    "    'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_800x1333/1',\n",
    "    'Faster R-CNN ResNet152 V1 640x640':\n",
    "    'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_640x640/1',\n",
    "    'Faster R-CNN ResNet152 V1 1024x1024':\n",
    "    'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_1024x1024/1',\n",
    "    'Faster R-CNN ResNet152 V1 800x1333':\n",
    "    'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_800x1333/1',\n",
    "    'Faster R-CNN Inception ResNet V2 640x640':\n",
    "    'https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_640x640/1',\n",
    "    'Faster R-CNN Inception ResNet V2 1024x1024':\n",
    "    'https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_1024x1024/1',\n",
    "    'Mask R-CNN Inception ResNet V2 1024x1024':\n",
    "    'https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1'\n",
    "}\n",
    "\n",
    "## total clases\n",
    "classes = {1: 'person', 2: 'bicycle', 3: 'car', 4: 'motorcycle', 5: 'airplane', 6: 'bus', 7: 'train', 8: 'truck', 9: 'boat', 10: 'traffic light', 11: 'fire hydrant', 13: 'stop sign', 14: 'parking meter', 15: 'bench', 16: 'bird', 17: 'cat', 18: 'dog', 19: 'horse', 20: 'sheep', 21: 'cow', 22: 'elephant', 23: 'bear', 24: 'zebra', 25: 'giraffe', 27: 'backpack', 28: 'umbrella', 31: 'handbag', 32: 'tie', 33: 'suitcase', 34: 'frisbee', 35: 'skis', 36: 'snowboard', 37: 'sports ball', 38: 'kite', 39: 'baseball bat', 40: 'baseball glove', 41: 'skateboard', 42: 'surfboard', 43: 'tennis racket', 44: 'bottle', 46: 'wine glass', 47: 'cup', 48: 'fork', 49: 'knife', 50: 'spoon', 51: 'bowl', 52: 'banana', 53: 'apple', 54: 'sandwich', 55: 'orange', 56: 'broccoli', 57: 'carrot', 58: 'hot dog', 59: 'pizza', 60: 'donut', 61: 'cake', 62: 'chair', 63: 'couch', 64: 'potted plant', 65: 'bed', 67: 'dining table', 70: 'toilet', 72: 'tv', 73: 'laptop', 74: 'mouse', 75: 'remote', 76: 'keyboard', 77: 'cell phone', 78: 'microwave', 79: 'oven', 80: 'toaster', 81: 'sink', 82: 'refrigerator', 84: 'book', 85: 'clock', 86: 'vase', 87: 'scissors', 88: 'teddy bear', 89: 'hair drier', 90: 'toothbrush'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'SSD MobileNet V2 FPNLite 320x320' # 'SSD MobileNet v2 320x320' #  ['CenterNet HourGlass104 512x512','CenterNet HourGlass104 Keypoints 512x512','CenterNet HourGlass104 1024x1024','CenterNet HourGlass104 Keypoints 1024x1024','CenterNet Resnet50 V1 FPN 512x512','CenterNet Resnet50 V1 FPN Keypoints 512x512','CenterNet Resnet101 V1 FPN 512x512','CenterNet Resnet50 V2 512x512','CenterNet Resnet50 V2 Keypoints 512x512','EfficientDet D0 512x512','EfficientDet D1 640x640','EfficientDet D2 768x768','EfficientDet D3 896x896','EfficientDet D4 1024x1024','EfficientDet D5 1280x1280','EfficientDet D6 1280x1280','EfficientDet D7 1536x1536','SSD MobileNet v2 320x320','SSD MobileNet V1 FPN 640x640','SSD MobileNet V2 FPNLite 320x320','SSD MobileNet V2 FPNLite 640x640','SSD ResNet50 V1 FPN 640x640 (RetinaNet50)','SSD ResNet50 V1 FPN 1024x1024 (RetinaNet50)','SSD ResNet101 V1 FPN 640x640 (RetinaNet101)','SSD ResNet101 V1 FPN 1024x1024 (RetinaNet101)','SSD ResNet152 V1 FPN 640x640 (RetinaNet152)','SSD ResNet152 V1 FPN 1024x1024 (RetinaNet152)','Faster R-CNN ResNet50 V1 640x640','Faster R-CNN ResNet50 V1 1024x1024','Faster R-CNN ResNet50 V1 800x1333','Faster R-CNN ResNet101 V1 640x640','Faster R-CNN ResNet101 V1 1024x1024','Faster R-CNN ResNet101 V1 800x1333','Faster R-CNN ResNet152 V1 640x640','Faster R-CNN ResNet152 V1 1024x1024','Faster R-CNN ResNet152 V1 800x1333','Faster R-CNN Inception ResNet V2 640x640','Faster R-CNN Inception ResNet V2 1024x1024','Mask R-CNN Inception ResNet V2 1024x1024']\n",
    "model_handle = ALL_MODELS[model_name]\n",
    "print('Selected model:'+ model_name)\n",
    "print('Model Handle at TensorFlow Hub: {}'.format(model_handle))\n",
    "detector = hub.load(model_handle)\n",
    "print('Model loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities\n",
    "\n",
    "Functions included in the [object-detection API](https://www.tensorflow.org/hub/tutorials/object_detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For drawing onto the image.\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from PIL import ImageColor\n",
    "from PIL import ImageDraw\n",
    "from PIL import ImageFont\n",
    "from PIL import ImageOps\n",
    "\n",
    "# For measuring the inference time.\n",
    "import time\n",
    "\n",
    "\n",
    "def draw_bounding_box_on_image(image,\n",
    "                               ymin,\n",
    "                               xmin,\n",
    "                               ymax,\n",
    "                               xmax,\n",
    "                               color,\n",
    "                               font,\n",
    "                               thickness=4,\n",
    "                               display_str_list=()):\n",
    "    \"\"\"Adds a bounding box to an image.\"\"\"\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    im_width, im_height = image.size\n",
    "    (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n",
    "                                  ymin * im_height, ymax * im_height)\n",
    "    draw.line([(left, top), (left, bottom), (right, bottom), (right, top),\n",
    "               (left, top)],\n",
    "              width=thickness,\n",
    "              fill=color)\n",
    "\n",
    "    # If the total height of the display strings added to the top of the bounding\n",
    "    # box exceeds the top of the image, stack the strings below the bounding box\n",
    "    # instead of above.\n",
    "    display_str_heights = [font.getsize(ds)[1] for ds in display_str_list]\n",
    "    # Each display_str has a top and bottom margin of 0.05x.\n",
    "    total_display_str_height = (1 + 2 * 0.05) * sum(display_str_heights)\n",
    "\n",
    "    if top > total_display_str_height:\n",
    "        text_bottom = top\n",
    "    else:\n",
    "        text_bottom = top + total_display_str_height\n",
    "    # Reverse list and print from bottom to top.\n",
    "    for display_str in display_str_list[::-1]:\n",
    "        text_width, text_height = font.getsize(display_str)\n",
    "        margin = np.ceil(0.05 * text_height)\n",
    "        draw.rectangle([(left, text_bottom - text_height - 2 * margin),\n",
    "                        (left + text_width, text_bottom)],\n",
    "                       fill=color)\n",
    "        draw.text((left + margin, text_bottom - text_height - margin),\n",
    "                  display_str,\n",
    "                  fill=\"black\",\n",
    "                  font=font)\n",
    "        text_bottom -= text_height - 2 * margin\n",
    "\n",
    "\n",
    "def draw_boxes(image, boxes, class_names, scores, max_boxes=10, min_score=0.45):\n",
    "    \"\"\"Overlay labeled boxes on an image with formatted scores and label names.\"\"\"\n",
    "    colors = list(ImageColor.colormap.values())\n",
    "\n",
    "    try:\n",
    "        font = ImageFont.truetype(\n",
    "            \"/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Regular.ttf\",\n",
    "            25)\n",
    "    except IOError:\n",
    "        #print(\"Font not found, using default font.\")\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    for i in range(min(boxes.shape[0], max_boxes)):\n",
    "        if scores[i] >= min_score:\n",
    "            ymin, xmin, ymax, xmax = tuple(boxes[i])\n",
    "            if not(i < len(class_names) and i < len(scores)):\n",
    "                continue\n",
    "            display_str = \"{}: {}%\".format(class_names[i],\n",
    "                                           int(100 * scores[i]))\n",
    "            color = colors[hash(class_names[i]) % len(colors)]\n",
    "            image_pil = Image.fromarray(np.uint8(image)).convert(\"RGB\")\n",
    "            draw_bounding_box_on_image(image_pil,\n",
    "                                       ymin,\n",
    "                                       xmin,\n",
    "                                       ymax,\n",
    "                                       xmax,\n",
    "                                       color,\n",
    "                                       font,\n",
    "                                       display_str_list=[display_str])\n",
    "            np.copyto(image, np.array(image_pil))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Image Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "id": "CO1JQobEXqai",
    "outputId": "a5c4fe4d-2e8b-4ec5-f7ac-64d893156500",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Detect one image\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "def read_image(image_path, target_size=None):\n",
    "    image = tf.keras.preprocessing.image.load_img(image_path,\n",
    "                target_size=target_size)\n",
    "    image = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    image = image.astype(np.uint8)\n",
    "    return image\n",
    "\n",
    "url = 'https://akm-img-a-in.tosshub.com/indiatoday/images/story/201812/dogs_and_cats.jpeg?TAxD19DTCFE7WiSYLUdTu446cfW4AbuW&size=770:433'\n",
    "image_path = tf.keras.utils.get_file(\"dog-cat2.jpg\", url)\n",
    "image = read_image(image_path)\n",
    "plt.imshow(image)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_img = np.expand_dims(image, 0)\n",
    "t = time.time()\n",
    "result = detector(converted_img)\n",
    "print(\"Inference time: \", time.time() - t)\n",
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['detection_scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_detector(detector, image, classes):\n",
    "\n",
    "    converted_img = np.expand_dims(image, 0)\n",
    "\n",
    "    start_time = time.time()\n",
    "    result = detector(converted_img)\n",
    "    end_time = time.time()\n",
    "\n",
    "    result = {key: value.numpy() for key, value in result.items()}\n",
    "    print(\"Inference time: \", end_time - start_time)\n",
    "\n",
    "    classes_names = [classes[i]\n",
    "                     for i in result[\"detection_classes\"][0] if i in classes]\n",
    "\n",
    "    detection_boxes = result[\"detection_boxes\"][0]\n",
    "    detection_scores = result[\"detection_scores\"][0]\n",
    "    image_with_boxes = draw_boxes(image, detection_boxes,\n",
    "                                  classes_names, detection_scores)\n",
    "\n",
    "    return image_with_boxes\n",
    "\n",
    "\n",
    "image_with_boxes = run_detector(detector, image, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "id": "RNooZ7QxYu_A",
    "outputId": "714b99b9-5460-4fec-b9ae-4b3338452541",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(image_with_boxes)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://i.ibb.co/R7pRTLy/beach-no-axis.png'\n",
    "image_path = tf.keras.utils.get_file(\"beach2.jpg\", url)\n",
    "image = read_image(image_path)\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(image)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_with_boxes = run_detector(detector, image, classes)\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(image_with_boxes)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://i.ibb.co/jL1kZRF/phones.png'\n",
    "image_path = tf.keras.utils.get_file(\"phones.jpg\", url)\n",
    "image = read_image(image_path)\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(image)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()\n",
    "\n",
    "image_with_boxes = run_detector(detector, image, classes)\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(image_with_boxes)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object detection with OpenCV, YOLO\n",
    "\n",
    "YOLO is a real-time object detection algorithm that is known for its speed and accuracy. Unlike other object detection algorithms that use a sliding window approach, YOLO divides the input image into an SxS grid and generates bounding boxes and class probabilities directly from the grid cells. This allows YOLO to process images in a single forward pass through the network, making it extremely fast and suitable for real-time applications.\n",
    "\n",
    "### Download the latest YOLO weights, configuration, and class names:\n",
    "\n",
    "Save these files from [AlexeyAB](https://github.com/AlexeyAB/darknet) in the same directory as your Jupyter Notebook.\n",
    "\n",
    "- [yolov4.weights](https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights)\n",
    "- [yolov4.cfg](https://github.com/AlexeyAB/darknet/blob/master/cfg/yolov4.cfg)\n",
    "- [coco.names](https://github.com/AlexeyAB/darknet/blob/master/data/coco.names)\n",
    "\n",
    "or in shell:\n",
    "\n",
    "```shell\n",
    "!wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights\n",
    "!wget https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov4.cfg\n",
    "!wget https://raw.githubusercontent.com/AlexeyAB/darknet/master/data/coco.names\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yolo_model():\n",
    "    # Load YOLO model configuration and weights\n",
    "    net = cv2.dnn.readNet(\"yolov4.weights\", \"yolov4.cfg\")\n",
    "\n",
    "    # Load COCO dataset classes\n",
    "    with open(\"coco.names\", \"r\") as f:\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    return net, classes\n",
    "\n",
    "\n",
    "def detect_objects(img, net):\n",
    "    blob = cv2.dnn.blobFromImage(img, 1/255, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    layer_names = net.getLayerNames()\n",
    "    output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "    outputs = net.forward(output_layers)\n",
    "    return outputs\n",
    "\n",
    "# Load the YOLO model\n",
    "net, classes = load_yolo_model()\n",
    "print(f'classes: {classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(img, detections, classes):\n",
    "    height, width, channels = img.shape\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    for output in detections:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    for i in indices:\n",
    "        i = i[0]\n",
    "        x, y, w, h = boxes[i]\n",
    "        label = str(classes[class_ids[i]])\n",
    "        color = np.random.randint(0, 255, 3).tolist()\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)\n",
    "        cv2.putText(img, label, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from urllib.error import HTTPError, URLError\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_image_from_url(url):\n",
    "    try:\n",
    "        with urllib.request.urlopen(url) as url_response:\n",
    "            image_array = np.asarray(bytearray(url_response.read()), dtype=np.uint8)\n",
    "            image = cv2.imdecode(image_array, -1)\n",
    "\n",
    "        # Convert the image from BGR to RGB (used by matplotlib)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        return image_rgb\n",
    "    except (HTTPError, URLError) as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://i.ibb.co/jL1kZRF/phones.png'\n",
    "image = read_image_from_url(url)\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(image)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()\n",
    "\n",
    "detections = detect_objects(image, net)\n",
    "image_with_boxes = draw_boxes(image, detections, classes)\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(image_with_boxes)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "time.sleep(1)  ### letting the camera autofocus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "axes = None\n",
    "NUM_FRAMES = 40  # you can change this\n",
    "processed_imgs = []\n",
    "for i in range(NUM_FRAMES):\n",
    "    # Load frame from the camera\n",
    "    ret, frame = cap.read()\n",
    "    detections = detect_objects(frame, net)\n",
    "    image_boxes = draw_boxes(frame, detections, classes)\n",
    "\n",
    "    img = Image.fromarray(np.uint8(image_boxes))#.convert('RGB')\n",
    "    processed_imgs.append(img)\n",
    "    cv2.imshow(\"test\", cv2.cvtColor(image_boxes, cv2.COLOR_BGR2RGB))\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create gif\n",
    "processed_imgs[0].save('web_cam.gif',\n",
    "                       format='GIF',\n",
    "                       append_images=processed_imgs[1:],\n",
    "                       save_all=True,\n",
    "                       duration=100,\n",
    "                       loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPyImage('web_cam.gif', format='png', width=15 * 40, height=3 * 40) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web cam Local\n",
    "#### Detection loop\n",
    "The detection loop consists of four phases:\n",
    "\n",
    "- loading the webcam frame\n",
    "\n",
    "- pre-processing the image\n",
    "\n",
    "- running the image through the network\n",
    "\n",
    "- updating the output with the resulting predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "# Load the webcam handler\n",
    "cap = cv2.VideoCapture(0)\n",
    "time.sleep(1)  ### letting the camera autofocus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "time.sleep(1)  ### letting the camera autofocus\n",
    "\n",
    "axes = None\n",
    "NUM_FRAMES = 100  # you can change this\n",
    "processed_imgs = []\n",
    "for i in range(NUM_FRAMES):\n",
    "    # Load frame from the camera\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image_boxes = run_detector(detector, frame, classes)\n",
    "    img = Image.fromarray(np.uint8(image_boxes))#.convert('RGB')\n",
    "    processed_imgs.append(img)\n",
    "    cv2.imshow(\"test\", cv2.cvtColor(image_boxes, cv2.COLOR_BGR2RGB))\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create gif\n",
    "processed_imgs[0].save('web_cam.gif',\n",
    "                       format='GIF',\n",
    "                       append_images=processed_imgs[1:],\n",
    "                       save_all=True,\n",
    "                       duration=100,\n",
    "                       loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IPyImage('web_cam.gif', format='png', width=15 * 40, height=3 * 40) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rv_lvY5uidpy"
   },
   "source": [
    "## Web cam Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iwBFg2e0Wg44"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "## Camera Capture\n",
    "Using a webcam to capture images for processing on the runtime.\n",
    "Source: https://colab.research.google.com/notebooks/snippets/advanced_outputs.ipynb#scrollTo=2viqYx97hPMi\n",
    "'''\n",
    "\n",
    "from IPython.display import display, Javascript\n",
    "from google.colab.output import eval_js\n",
    "from base64 import b64decode\n",
    "\n",
    "def take_photo(filename='photo.jpg', quality=0.8):\n",
    "  js = Javascript('''\n",
    "    async function takePhoto(quality) {\n",
    "      const div = document.createElement('div');\n",
    "      const video = document.createElement('video');\n",
    "      video.style.display = 'block';\n",
    "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
    "\n",
    "      // show the video in the HTML element\n",
    "      document.body.appendChild(div);\n",
    "      div.appendChild(video);\n",
    "      video.srcObject = stream;\n",
    "      await video.play();\n",
    "\n",
    "      // Resize the output to fit the video element.\n",
    "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
    "\n",
    "      // prints the logs to cell\n",
    "      let jsLog = function(abc) {\n",
    "        document.querySelector(\"#output-area\").appendChild(document.createTextNode(`${abc}... `));\n",
    "      }\n",
    "\n",
    "      // Wait for Capture to be clicked.\n",
    "      // await new Promise((resolve) => capture.onclick = resolve);\n",
    "\n",
    "      for (let i = 0; i < 5; i++) {\n",
    "        const canvas = document.createElement('canvas');\n",
    "        canvas.width = video.videoWidth;\n",
    "        canvas.height = video.videoHeight;\n",
    "        canvas.getContext('2d').drawImage(video, 0, 0);\n",
    "        img = canvas.toDataURL('image/jpeg', quality);\n",
    "\n",
    "        // show each captured image\n",
    "        // let imgTag = document.createElement('img');\n",
    "        // imgTag.src = img;\n",
    "        // div.appendChild(imgTag);\n",
    "\n",
    "        jsLog(i + \"sending\")\n",
    "        // Call a python function and send this image\n",
    "        google.colab.kernel.invokeFunction('notebook.run_algo', [img], {});\n",
    "        jsLog(i + \"SENT\")\n",
    "        // wait for X miliseconds second, before next capture\n",
    "        await new Promise(resolve => setTimeout(resolve, 250));\n",
    "      }\n",
    "\n",
    "      stream.getVideoTracks()[0].stop(); // stop video stream\n",
    "    }\n",
    "    ''')\n",
    "  display(js) # make the provided HTML, part of the cell\n",
    "  data = eval_js('takePhoto({})'.format(quality)) # call the takePhoto() JavaScript function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "SVaSNkSkiiB9",
    "outputId": "d04a4a95-92d9-4476-e91d-0ab6007c797d"
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "from google.colab import output\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import logging\n",
    "\n",
    "def data_uri_to_img(uri):\n",
    "    \"\"\"convert base64image to numpy array\"\"\"\n",
    "    try:\n",
    "        image = base64.b64decode(uri.split(',')[1], validate=True)\n",
    "        # make the binary image, a PIL image\n",
    "        image = Image.open(BytesIO(image))\n",
    "        # convert to numpy array\n",
    "        image = np.array(image, dtype=np.uint8);\n",
    "        return image\n",
    "    except Exception as e:\n",
    "        logging.exception(e);\n",
    "        print('\\n')\n",
    "        return None\n",
    "\n",
    "\n",
    "def run_algo(imgB64):\n",
    "    \"\"\"\n",
    "    in Colab, run_algo function gets invoked by the JavaScript, that sends N images every second\n",
    "  \n",
    "    params:\n",
    "      image: image\n",
    "    \"\"\"\n",
    "    image = data_uri_to_img(imgB64)\n",
    "    frame = image\n",
    "    if image is None:\n",
    "        print(\"At run_algo(): image is None.\")\n",
    "        return\n",
    "    try:\n",
    "        # Run detection\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB).astype('uint8')\n",
    "        image_boxes = run_detector(detector, frame, classes)\n",
    "        cv2_imshow(image_boxes)\n",
    "    except Exception as e:\n",
    "        logging.exception(e)\n",
    "\n",
    "# register this function, so JS code could call this\n",
    "output.register_callback('notebook.run_algo', run_algo)\n",
    "\n",
    "# put the JS code in cell and run it\n",
    "take_photo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice: Traffic Scene  Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scene 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the scene gif\n",
    "url_1 = 'https://i.ibb.co/wpHvb58/scene1.gif'\n",
    "scene_1_path = tf.keras.utils.get_file(\"scene_1.gif\", url_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IPyImage(scene_1_path, format='png', width=15 * 40, height=3 * 40)\n",
    "#IPyImage(url=url_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gif_object = Image.open(scene_1_path)\n",
    "\n",
    "# Display individual frames from the loaded animated GIF file\n",
    "processed_imgs = []\n",
    "for ind in range(0, gif_object.n_frames):\n",
    "    gif_object.seek(ind)\n",
    "    ##\u00a0frame in numpy array format (512, 512, 3)\n",
    "    frame = np.array(gif_object.convert('RGB'))\n",
    "    ## Object Detection code\n",
    "\n",
    "    ##\n",
    "    processed_imgs.append(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the processed scene gif\n",
    "processed_imgs[0].save('scene1_boxes.gif',\n",
    "                       format='GIF',\n",
    "                       append_images=processed_imgs[1:],\n",
    "                       save_all=True,\n",
    "                       duration=200,\n",
    "                       loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPyImage('scene1_boxes.gif', format='png', width=15 * 40, height=3 * 40) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download from colab\n",
    "#from google.colab import files\n",
    "#files.download('scene1_boxes.gif') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scene 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the scene gif\n",
    "url_2 = 'https://i.ibb.co/wpHvb58/scene2.gif'\n",
    "scene_2_path = tf.keras.utils.get_file(\"scene_2.gif\", url_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IPyImage(scene_2_path, format='png', width=15 * 40, height=3 * 40)\n",
    "#IPyImage(url=url_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gif_object = Image.open(scene_2_path)\n",
    "\n",
    "# Display individual frames from the loaded animated GIF file\n",
    "processed_imgs = []\n",
    "for ind in range(0, gif_object.n_frames):\n",
    "    gif_object.seek(ind)\n",
    "    ##\u00a0frame in numpy array format (512, 512, 3)\n",
    "    frame = np.array(gif_object.convert('RGB'))\n",
    "    ## Object Detection code\n",
    "\n",
    "    ##\n",
    "    processed_imgs.append(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the processed scene gif\n",
    "processed_imgs[0].save('scene2_boxes.gif',\n",
    "                       format='GIF',\n",
    "                       append_images=processed_imgs[1:],\n",
    "                       save_all=True,\n",
    "                       duration=200,\n",
    "                       loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPyImage('scene2_boxes.gif', format='png', width=15 * 40, height=3 * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download from colab\n",
    "#from google.colab import files\n",
    "#files.download('scene1_boxes.gif')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "YOLO_webcam.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}