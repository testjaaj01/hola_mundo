{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is highly recommended to use a powerful **GPU**, you can use it for free uploading this notebook to [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb).\n",
    "<table align=\"center\">\n",
    " <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/ezponda/intro_deep_learning/blob/main/class/CNN/Neural_Style_Transfer.ipynb\">\n",
    "        <img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
    "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/ezponda/intro_deep_learning/blob/main/class/CNN/Neural_Style_Transfer.ipynb\">\n",
    "        <img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />View Source on GitHub</a></td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-8BW8xSfK0d"
   },
   "source": [
    "# Neural style transfer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmp3MxXVfK0k"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "Style transfer consists in generating an image merging a **\"content\"(C)** image with a **\"style\"(S)** image to create a **\"generated\"(G)** image. The generated image G combines the \"content\" of the image C with the \"style\" of image S (generally artistic). \n",
    "\n",
    "\n",
    "This is achieved through the optimization of a loss function\n",
    "that has 3 components: \"style loss\", \"content loss\",\n",
    "and \"total variation loss\":\n",
    "\n",
    "- **Total variation loss**: Imposes local spatial continuity between\n",
    "the pixels of the combination image, giving it visual coherence.\n",
    "- **Style loss**: is where the deep learning keeps in --that one is defined\n",
    "using a deep convolutional neural network. Precisely, it consists in a sum of\n",
    "L2 distances between the Gram matrices of the representations of\n",
    "the base image and the style reference image, extracted from\n",
    "different layers of a convnet (trained on ImageNet). The general idea\n",
    "is to capture color/texture information at different spatial\n",
    "scales (fairly large scales --defined by the depth of the layer considered).\n",
    "- **Content loss**: is a L2 distance between the features of the base\n",
    "image (extracted from a deep layer) and the features of the combination image,\n",
    "keeping the generated image close enough to the original one.\n",
    "\n",
    "**References:** \n",
    "- [A Neural Algorithm of Artistic Style](\n",
    "  http://arxiv.org/abs/1508.06576)\n",
    "- Same notebook as [Chollet](https://keras.io/examples/generative/neural_style_transfer/)\n",
    "- Course 4, week 4 (convolutional neural networks) of Coursera deep learning specialization, [link](https://www.coursera.org/learn/convolutional-neural-networks?specialization=deep-learning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OX1TJ1CPRo1"
   },
   "source": [
    "It is highly recommended to use a powerful **GPU**, you can use it for free uploading this notebook to [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MptO5XwbPRo1"
   },
   "outputs": [],
   "source": [
    "## if you have a GPU\n",
    "GPU=False\n",
    "#%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "if GPU:\n",
    "    device_name = tf.test.gpu_device_name()\n",
    "    if device_name != '/device:GPU:0':\n",
    "        raise SystemError('GPU device not found')\n",
    "    print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1Em5sBYfK0l"
   },
   "source": [
    "## Loading images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dWnYBmqdPRo2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5RDthKZfgNO",
    "outputId": "04862efd-bb9f-47d0-94f2-f7a1f040a236"
   },
   "source": [
    "You can download the image with an image url using [get_file](https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_file)\n",
    "```python\n",
    "tf.keras.utils.get_file(\n",
    "    fname, origin, untar=False, md5_hash=None, file_hash=None,\n",
    "    cache_subdir='datasets', hash_algorithm='auto',\n",
    "    extract=False, archive_format='auto', cache_dir=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a2I49qazQXuz"
   },
   "outputs": [],
   "source": [
    "result_prefix = \"paris_generated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MRgElchRPRo3"
   },
   "outputs": [],
   "source": [
    "## Content (C) Image\n",
    "url_content = \"https://i.imgur.com/F28w3Ac.jpg\"\n",
    "# url_content = 'https://lp-cms-production.imgix.net/2020-11/GettyRF_494057771.jpg?auto=format&fit=crop&sharp=10&vib=20&ixlib=react-8.6.4&w=850&q=50&dpr=2'\n",
    "base_image_path = tf.keras.utils.get_file(\"paris.jpg\", url_content)\n",
    "display(Image(base_image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ocZXexPUPRo4"
   },
   "outputs": [],
   "source": [
    "## Style (S) Image\n",
    "url_style = 'https://i.imgur.com/9ooB60I.jpg'\n",
    "style_reference_image_path = keras.utils.get_file(\n",
    "    \"starry_night.jpg\", url_style)\n",
    "\n",
    "display(Image(style_reference_image_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zb3lWHZmPRo4"
   },
   "source": [
    "## Transfer Learning\n",
    "\n",
    "We are going to use the same pre-trained CNN as in the [original paper](https://arxiv.org/abs/1508.06576) the [VGG-19](https://keras.io/api/applications/vgg/), a 19-layer version of the VGG network trained in ImageNet.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Hqu3pS6PRo4"
   },
   "outputs": [],
   "source": [
    "# Build a VGG19 model loaded with pre-trained ImageNet weights\n",
    "from tensorflow.keras.applications import vgg19\n",
    "model = vgg19.VGG19(weights=\"imagenet\", include_top=False)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TdvgjZ9vPRo5"
   },
   "outputs": [],
   "source": [
    "# Get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "outputs_dict = {layer.name:layer.output for layer in model.layers}\n",
    "# Set up a model that returns the activation values for every layer in\n",
    "# VGG19 (as a dict).\n",
    "feature_extractor = keras.Model(inputs=model.inputs, outputs=outputs_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5T4qUvAPRo5"
   },
   "source": [
    "We need to define the layers that we are going to use in the NST, you can play with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t2WgKbPiPRo5"
   },
   "outputs": [],
   "source": [
    "# List of layers to use for the style loss.\n",
    "style_layer_names = [\n",
    "    \"block1_conv1\",\n",
    "    \"block2_conv1\",\n",
    "    \"block3_conv1\",\n",
    "    \"block4_conv1\",\n",
    "    \"block5_conv1\",\n",
    "]\n",
    "# The layer to use for the content loss.\n",
    "content_layer_name = \"block5_conv2\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3V6bnFDPRo5"
   },
   "source": [
    "Lets visualize one feature map for the content image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9HGmFhHLPRo5"
   },
   "outputs": [],
   "source": [
    "img_C = keras.preprocessing.image.load_img(base_image_path)\n",
    "img_C = tf.keras.preprocessing.image.img_to_array(img_C)\n",
    "img_C = np.expand_dims(img_C, axis=0)\n",
    "img_C = tf.keras.applications.vgg19.preprocess_input(img_C)\n",
    "features = feature_extractor(img_C)\n",
    "layer_features = features[content_layer_name]\n",
    "print(layer_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hzXx_ANMPRo6"
   },
   "outputs": [],
   "source": [
    "## features from content layer\n",
    "ind = np.random.randint(512)\n",
    "plt.imshow(layer_features[0, :, :, ind], cmap='viridis')\n",
    "plt.title('Content Layer Feature, channel:{0}'.format(ind))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4AeJqsSPRo6"
   },
   "source": [
    "Same with the style features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iuvxESMBPRo6"
   },
   "outputs": [],
   "source": [
    "img_S = keras.preprocessing.image.load_img(style_reference_image_path)\n",
    "img_S = tf.keras.preprocessing.image.img_to_array(img_S)\n",
    "img_S = np.expand_dims(img_S, axis=0)\n",
    "img_S = tf.keras.applications.vgg19.preprocess_input(img_S)\n",
    "features = feature_extractor(img_S)\n",
    "for style_layer in style_layer_names:\n",
    "    print(style_layer)\n",
    "    layer_features = features[style_layer]\n",
    "    ind = np.random.randint(layer_features.shape[-1])\n",
    "    plt.imshow(layer_features[0, :, :, ind], cmap='viridis')\n",
    "    plt.title('Style Layer Feature, channel:{0}'.format(ind))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTEPKb7dfK0n"
   },
   "source": [
    "## Image preprocessing / deprocessing utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3OCsLqhufK0o"
   },
   "outputs": [],
   "source": [
    "# Dimensions of the generated picture.\n",
    "width, height = keras.preprocessing.image.load_img(base_image_path).size\n",
    "img_nrows = 400\n",
    "img_ncols = int(width * img_nrows / height)\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    # Util function to open, resize and format pictures into appropriate tensors\n",
    "    img = keras.preprocessing.image.load_img(\n",
    "        image_path, target_size=(img_nrows, img_ncols)\n",
    "    )\n",
    "    img = keras.preprocessing.image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = vgg19.preprocess_input(img)\n",
    "    return tf.convert_to_tensor(img)\n",
    "\n",
    "\n",
    "def deprocess_image(x):\n",
    "    # Util function to convert a tensor into a valid image\n",
    "    x = x.reshape((img_nrows, img_ncols, 3))\n",
    "    # Remove zero-center by mean pixel\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    # 'BGR'->'RGB'\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype(\"uint8\")\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3PIdhHd-fK0o"
   },
   "source": [
    "## Compute losses functions\n",
    "\n",
    "First, we need to define 4 utility functions:\n",
    "\n",
    "- `gram_matrix` (used to compute the style loss)\n",
    "- The `style_loss` function, which keeps the generated image close to the local textures\n",
    "of the style reference image\n",
    "- The `content_loss` function, which keeps the high-level representation of the\n",
    "generated image close to that of the base image\n",
    "- The `total_variation_loss` function, a regularization loss which keeps the generated\n",
    "image locally-coherent\n",
    "\n",
    "\n",
    "#### Content Cost Function $J_{content}(C,G)$\n",
    "\n",
    "The content loss is a L2 distance between the features of the base\n",
    "image (extracted from a deep layer) and the features of the combination image,\n",
    "keeping the generated image close enough to the original one.\n",
    "\n",
    "We will define the content cost function as:\n",
    "\n",
    "$$J_{content}(C,G) =  \\frac{1}{4 \\times n_H \\times n_W \\times n_C} \\sum _{ \\text{all entries}} (a^{(C)} - a^{(G)})^2 $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CmX74cBJPRo8"
   },
   "outputs": [],
   "source": [
    "def content_loss(base, combination):\n",
    "    n_H, n_W, n_C = base.shape \n",
    "    norm = 4 * n_H * n_W * n_C\n",
    "    return tf.reduce_sum(tf.square(combination - base)) / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V20gIXprPRo8"
   },
   "source": [
    "### Style Cost Function $J_{style}(S,G)$. \n",
    "\n",
    "$$J_{style}^{[l]}(S,G) = \\frac{1}{4 \\times {n_C}^2 \\times (n_H \\times n_W)^2} \\sum _{i=1}^{n_C}\\sum_{j=1}^{n_C}(G^{(S)}_{(gram)i,j} - G^{(G)}_{(gram)i,j})^2\\tag{2} $$\n",
    "\n",
    "* $G_{gram}^{(S)}$ Gram matrix of the \"style\" image.\n",
    "* $G_{gram}^{(G)}$ Gram matrix of the \"generated\" image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YFQq0MBgPRo8"
   },
   "outputs": [],
   "source": [
    "# The gram matrix of an image tensor (feature-wise outer product)\n",
    "\n",
    "def gram_matrix(x):\n",
    "    x = tf.transpose(x, (2, 0, 1))\n",
    "    features = tf.reshape(x, (tf.shape(x)[0], -1))\n",
    "    gram = tf.matmul(features, tf.transpose(features)) \n",
    "    return gram\n",
    "\n",
    "\n",
    "# The \"style loss\" is designed to maintain\n",
    "# the style of the reference image in the generated image.\n",
    "# It is based on the gram matrices (which capture style) of\n",
    "# feature maps from the style reference image\n",
    "# and from the generated image\n",
    "\n",
    "def style_loss(style, combination):\n",
    "    S = gram_matrix(style)\n",
    "    C = gram_matrix(combination)\n",
    "    n_H, n_W, n_C = style.shape \n",
    "    size = n_H *  n_W\n",
    "    norm = (4.0 * (n_C ** 2) * (size ** 2))\n",
    "    return tf.reduce_sum(tf.square(S - C)) / norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1j_rnD78PRo8"
   },
   "source": [
    "### Total Variation Loss $J_{TV}(G)$\n",
    "\n",
    "$$ J_{TV}(G) = \\sum_{i,j} \\sqrt{|G_{i+1,j} - G_{i,j}|^2 + |G_{i,j+1} - G_{i,j}|^2} $$\n",
    "\n",
    "We are going to use a small modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jgeZvcYbfK0o"
   },
   "outputs": [],
   "source": [
    "# The 3rd loss function, total variation loss,\n",
    "\n",
    "def total_variation_loss(x):\n",
    "    return tf.image.total_variation(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCS_XThgPRo9"
   },
   "source": [
    "### Complete loss \n",
    "\n",
    "$$ J(G) = \\lambda_{content} \\cdot J_{content}(C,G) + \\lambda_{style} \\cdot J_{style}(S,G) + \\lambda_{TV} \\cdot J_{TV}(G) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i7ZjwtJUfK0q"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Weights of the different loss components\n",
    "total_variation_weight = 1e-6\n",
    "style_weight = 1e-6\n",
    "content_weight = 2.5e-8\n",
    "\n",
    "\n",
    "def compute_loss(combination_image, base_image, style_reference_image,\n",
    "                 total_variation_weight, style_weight, content_weight):\n",
    "    input_tensor = tf.concat(\n",
    "        [base_image, style_reference_image, combination_image], axis=0\n",
    "    )\n",
    "    features = feature_extractor(input_tensor)\n",
    "\n",
    "    # Initialize the loss\n",
    "    loss = tf.zeros(shape=())\n",
    "\n",
    "    # Add content loss\n",
    "    layer_features = features[content_layer_name]\n",
    "    base_image_features = layer_features[0, :, :, :]\n",
    "    combination_features = layer_features[2, :, :, :]\n",
    "    loss = loss + content_weight * content_loss(\n",
    "        base_image_features, combination_features\n",
    "    )\n",
    "    # Add style loss\n",
    "    for layer_name in style_layer_names:\n",
    "        layer_features = features[layer_name]\n",
    "        style_reference_features = layer_features[1, :, :, :]\n",
    "        combination_features = layer_features[2, :, :, :]\n",
    "        sl = style_loss(style_reference_features, combination_features)\n",
    "        loss += (style_weight / len(style_layer_names)) * sl\n",
    "\n",
    "    # Add total variation loss\n",
    "    loss += total_variation_weight * total_variation_loss(combination_image)\n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def compute_loss_separate(combination_image, base_image, style_reference_image,\n",
    "                          total_variation_weight, style_weight, content_weight):\n",
    "    input_tensor = tf.concat(\n",
    "        [base_image, style_reference_image, combination_image], axis=0\n",
    "    )\n",
    "    features = feature_extractor(input_tensor)\n",
    "\n",
    "    # Add content loss\n",
    "    c_loss = tf.zeros(shape=())\n",
    "    layer_features = features[content_layer_name]\n",
    "    base_image_features = layer_features[0, :, :, :]\n",
    "    combination_features = layer_features[2, :, :, :]\n",
    "    c_loss = c_loss +  content_loss(\n",
    "        base_image_features, combination_features\n",
    "    )\n",
    "    # Add style loss\n",
    "    s_loss = tf.zeros(shape=())\n",
    "    for layer_name in style_layer_names:\n",
    "        layer_features = features[layer_name]\n",
    "        style_reference_features = layer_features[1, :, :, :]\n",
    "        combination_features = layer_features[2, :, :, :]\n",
    "        sl = style_loss(style_reference_features, combination_features)\n",
    "        s_loss += (1 / len(style_layer_names)) * sl\n",
    "\n",
    "    tv_loss = tf.zeros(shape=())\n",
    "    tv_loss = tv_loss + total_variation_loss(combination_image)\n",
    "\n",
    "    return c_loss, s_loss, tv_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vG6QfCHfK0r"
   },
   "source": [
    "## Add a tf.function decorator to loss & gradient computation\n",
    "\n",
    "To compile it, and thus make it fast.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pXC4cXZGfK0s"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_loss_and_grads(combination_image, base_image, style_reference_image,\n",
    "                           total_variation_weight, style_weight, content_weight):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(combination_image, base_image, style_reference_image,\n",
    "        total_variation_weight, style_weight, content_weight)\n",
    "    grads = tape.gradient(loss, combination_image)\n",
    "    return loss, grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNdpaouEfK0t"
   },
   "source": [
    "## The training loop\n",
    "\n",
    "Repeatedly run vanilla gradient descent steps to minimize the loss, and save the\n",
    "resulting image every 100 iterations.\n",
    "\n",
    "We decay the learning rate by 0.96 every 100 steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FzMBXuDYfK0t"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(\n",
    "    keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=100.0, decay_steps=100, decay_rate=0.96\n",
    "    )\n",
    ")\n",
    "\n",
    "total_variation_weight = 1e-6\n",
    "style_weight = 1e-5\n",
    "content_weight = 2.5e-6\n",
    "\n",
    "base_image = preprocess_image(base_image_path)\n",
    "style_reference_image = preprocess_image(style_reference_image_path)\n",
    "combination_image = tf.Variable(preprocess_image(base_image_path))\n",
    "\n",
    "iterations = 4000\n",
    "for i in range(1, iterations + 1):\n",
    "    loss, grads = compute_loss_and_grads(\n",
    "        combination_image, base_image, style_reference_image,\n",
    "        total_variation_weight, style_weight, content_weight\n",
    "    )\n",
    "    optimizer.apply_gradients([(grads, combination_image)])\n",
    "    if i % 5 == 0:\n",
    "        print(\"Iteration %d: loss=%.2f\" % (i, loss))\n",
    "        c_loss, s_loss, tv_loss = compute_loss_separate(\n",
    "            combination_image, base_image, style_reference_image,\n",
    "            total_variation_weight, style_weight, content_weight)\n",
    "        print('content_loss={0}, style_loss={1}, tv_loss={2}'.format(c_loss, s_loss, tv_loss))\n",
    "        print('weighed content_loss={0}, style_loss={1}, tv_loss={2}'.format(\n",
    "            content_weight * c_loss, style_weight * s_loss, total_variation_weight * tv_loss))\n",
    "        img = deprocess_image(combination_image.numpy())\n",
    "        fname = result_prefix + \"_at_iteration_%d.png\" % i\n",
    "        #keras.preprocessing.image.save_img(fname, img)\n",
    "        #display(Image(fname))\n",
    "        plt.imshow(img)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.preprocessing.image.save_img(fname, img)\n",
    "display(Image(fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McfK0ETAfK0t"
   },
   "source": [
    "After 4000 iterations, you get the following result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C2ByU_WLfK0t"
   },
   "outputs": [],
   "source": [
    "display(Image(result_prefix + \"_at_iteration_4000.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img);\n",
    "plt.xticks([]);\n",
    "plt.yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast Style Transfer using TF-Hub\n",
    "\n",
    "[Exploring the structure of a real-time, arbitrary neural artistic stylization network](https://arxiv.org/abs/1705.06830) (Ghiasi, Lee, Kudlur, Dumoulin & Shlens,\n",
    "  2017)\n",
    "\n",
    "Let's see how the [TensorFlow Hub model](https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2) implements the fast style transfer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "hub_model = hub.load('https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2')\n",
    "stylized_image = hub_model(tf.constant(img_C), tf.constant(img_S))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "def tensor_to_image(tensor):\n",
    "    tensor = tensor*255\n",
    "    tensor = np.array(tensor, dtype=np.uint8)\n",
    "    if np.ndim(tensor) > 3:\n",
    "        assert tensor.shape[0] == 1\n",
    "        tensor = tensor[0]\n",
    "    return PIL.Image.fromarray(tensor)\n",
    "tensor_to_image(stylized_image)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Neural Style Tansfer.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}