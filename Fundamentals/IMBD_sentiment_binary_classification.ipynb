{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is highly recommended to use a powerful **GPU**, you can use it for free uploading this notebook to [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb).\n",
    "<table align=\"center\">\n",
    " <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/ezponda/intro_deep_learning/blob/main/class/Fundamentals/IMBD_sentiment_binary_classification.ipynb\">\n",
    "        <img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
    "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/ezponda/intro_deep_learning/blob/main/class/Fundamentals/IMBD_sentiment_binary_classification.ipynb\">\n",
    "        <img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />View Source on GitHub</a></td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Example\n",
    " Two-class classification, or binary classification, may be the most widely applied kind of machine-learning problem. In this example, you’ll learn to classify movie reviews as positive or negative, based on the text content of the reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset: The IMDB dataset\n",
    "We’ll work with the IMDB dataset: a set of 50,000 highly polarized reviews from the Internet Movie Database. They’re split into 25,000 reviews for training and 25,000 reviews for testing, each set consisting of 50% negative and 50% positive reviews. The  parameter `num_words` controls how many words different we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "num_words = 5000\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=num_words)\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform word_id to word and reverse\n",
    "word2int = imdb.get_word_index()\n",
    "word2int = {w: i+3 for w, i in word2int.items()}\n",
    "word2int[\"<PAD>\"] = 0\n",
    "word2int[\"<START>\"] = 1\n",
    "word2int[\"<UNK>\"] = 2\n",
    "word2int[\"<UNUSED>\"] = 3\n",
    "int2word = {i: w for w, i in word2int.items()}\n",
    "num_words = num_words+3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For transforming an id-sequence to a phrase use get_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(sentence, int2word):\n",
    "    return ' '.join([int2word.get(i,'<UNK>') for i in sentence])\n",
    "print(get_words(train_data[0], int2word))\n",
    "print('Sentiment: ', train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "You need to convert your raw text to an appropriate input to a sequential model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text_sentence(text, word2int):\n",
    "    tokens = text.split(' ')\n",
    "    tokens_id = [word2int.get(tk,2) for tk in tokens]\n",
    "    return tokens_id\n",
    "\n",
    "text = get_words(train_data[0], int2word)\n",
    "print(text)\n",
    "print(vectorize_text_sentence(text, word2int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words Model BoW\n",
    "We are going to use a bag of words model. BoW is a simplifying representation used in natural language processing. In this model, a text (such as a sentence or a document) is represented as the Each key is the word, and each value is the frequency of occurrences of that word in the given text document.\n",
    "\n",
    "- **Input document**: `\"John likes to watch movies Mary likes movies too\"`\n",
    "- **BoW**: `{'John': 1, 'likes': 2, 'to': 1, 'watch': 1, 'movies': 2, 'Mary': 1, 'too': 1}`\n",
    "- **BoW Normalized**: `{'John': 0.11, 'likes': 0.22, 'to': 0.11, 'watch': 0.11, 'movies': 0.22, 'Mary': 0.11, 'too': 0.11}`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_bag_of_words(sequence, norm=True):\n",
    "    word_count = Counter(sequence)\n",
    "    if norm:\n",
    "        total = sum(word_count.values())\n",
    "        word_freq = {w: n / total for w, n in word_count.items()}\n",
    "        return word_freq\n",
    "    else:\n",
    "        return dict(word_count.items())\n",
    "\n",
    "text_example = \"John likes to watch movies Mary likes movies too\"\n",
    "print('text_example', text_example)\n",
    "text_sequence = text_example.split()\n",
    "print('text splitted', text_sequence)\n",
    "bag_of_words = get_bag_of_words(text_sequence)\n",
    "print('bag_of_words', bag_of_words)\n",
    "print('bag_of_words norm=False', get_bag_of_words(text_sequence, norm=False))\n",
    "print(\n",
    "    'bag_of_words with indexes', {\n",
    "        word2int[w.lower()]: p\n",
    "        for w, p in get_bag_of_words(text_sequence, norm=False).items()\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Term Matrix\n",
    "\n",
    "We need a way to model the documents so that they are all the same length, so that we can use a neural network. For this we are going to use the document term matrix.\n",
    "- Every document is a vector with the dimension of the vocabulary.\n",
    "- The position i of the vector corresponds to the word with index i.\n",
    "- The vector is all zeros except for the BoW word positions, which are filled with the frequency of the corresponding word.\n",
    "\n",
    "\n",
    "For example:\n",
    "\n",
    "- D1 = \"I like movies\",  `{'I': 1, 'like': 1, 'movies': 1}`\n",
    "\n",
    "- D2 = \"I dislike movies\",  `{'I': 1, 'dislike': 1, 'movies': 1}`\n",
    "\n",
    "Then the document-term matrix would be:\n",
    "\n",
    "\n",
    "\n",
    "   | Doc| I  | like    | dislike   | movies   |\n",
    "|---:|:-------------|:-----------|:------|:------|\n",
    "| D1 | 1  | 1       | 0   | 1     |\n",
    "| D2 | 1  | 0    | 1   | 1     |\n",
    "\n",
    "$D1 = [1,1,0,1]$\n",
    "\n",
    "$D2 = [1,0,1,1]$\n",
    "\n",
    "We convert every BoW to a vector of `dim=num_words` with `vectorize_sequences`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequence(sequence, num_words, norm=True):\n",
    "    vec = np.zeros(num_words)\n",
    "    bow = get_bag_of_words(sequence, norm)\n",
    "    for w, freq in bow.items():\n",
    "        if w < num_words:\n",
    "            vec[w] = freq\n",
    "    return vec\n",
    "\n",
    "\n",
    "def vectorize_sequences(sequences, num_words=num_words, norm=True):\n",
    "    \"\"\"Creates an all-zero matrix of shape (len(sequences), num_words)\"\"\"\n",
    "    results = np.zeros((len(sequences), num_words))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, :] = vectorize_sequence(sequence, num_words, norm)\n",
    "    return results\n",
    "\n",
    "\n",
    "x_train = vectorize_sequences(train_data, num_words=num_words)\n",
    "x_test = vectorize_sequences(test_data, num_words=num_words)\n",
    "y_train =np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train a model \n",
    "\n",
    "Define, compile and fit your NN model\n",
    "\n",
    "1. You can use the [Functional API](https://keras.io/guides/functional_api/):\n",
    "\n",
    "You need to start with an input data entry:\n",
    "```python    \n",
    "    inputs = keras.Input(shape=(8,))\n",
    "    layer_1 = layers.Dense(...)(inputs)\n",
    "```\n",
    "\n",
    "and the network outputs:\n",
    "```python\n",
    "outputs = layers.Dense(...)(previous_layer)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "```\n",
    "\n",
    "2. Or you can use [Sequential API](https://keras.io/guides/sequential_model/)\n",
    "\n",
    "```python\n",
    "model = keras.Sequential(name='example_model')\n",
    "model.add(layers.Dense(..., input_shape=(8,))\n",
    "model.add(...\n",
    "```\n",
    "\n",
    "You can introduce regularization methods seen in [Prevent_Overfitting.ipynb](https://github.com/ezponda/intro_deep_learning/blob/main/class/Fundamentals/Prevent_Overfitting.ipynb) like [Dropout layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout):\n",
    "\n",
    "\n",
    "```python\n",
    "tf.keras.layers.Dropout(\n",
    "    rate, noise_shape=None, seed=None, **kwargs\n",
    ")\n",
    "```\n",
    "\n",
    "With Functional API:\n",
    "```python\n",
    "next_layer = layers.Dropout(0.4)(prev_layer)\n",
    "```\n",
    "With Sequential:\n",
    "```python\n",
    "model.add(layers.Dropout(0.4))\n",
    "```\n",
    "\n",
    "First try with only one hidden layer and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(...,), name='input_layer')  # entrada\n",
    "\n",
    "# Model definition\n",
    "model = keras.Model(inputs=..., outputs=...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # can be 'val_accuracy'\n",
    "    patience=5,  # if during 5 epochs there is no improvement in `val_loss`, the execution will stop\n",
    "    restore_best_weights=True,\n",
    "    verbose=1)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "history = model.fit(x_train, y_train, validation_split=0.25, epochs= 10, batch_size=32, callbacks=[es_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "def show_loss_accuracy_evolution(hist):\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Sparse Categorical Crossentropy')\n",
    "    ax1.plot(hist['epoch'], hist['loss'], label='Train Error')\n",
    "    ax1.plot(hist['epoch'], hist['val_loss'], label = 'Val Error')\n",
    "    ax1.grid()\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.plot(hist['epoch'], hist['accuracy'], label='Train Accuracy')\n",
    "    ax2.plot(hist['epoch'], hist['val_accuracy'], label = 'Val Accuracy')\n",
    "    ax2.grid()\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "show_loss_accuracy_evolution(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model\n",
    "You need to obtain a Test Accuracy > 0.85. Try to get more than 0.9!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_errors(x_test, model, labels, int2word, n_samples=10):\n",
    "    preds = 1.0 * (model.predict(x_test).flatten() > 0.5)\n",
    "    bad_pred_inds = np.where(preds != labels)[0]\n",
    "    n_samples = min(len(bad_pred_inds), n_samples)\n",
    "    samples_inds = np.random.choice(bad_pred_inds, n_samples)\n",
    "    for ind in samples_inds:\n",
    "        print('Predicted : {0}, real : {1}, lenght: {2}'.format(\n",
    "            int(preds[ind]), labels[ind], len(test_data[ind])))\n",
    "        print(get_words(test_data[ind], int2word))\n",
    "        print()\n",
    "    return\n",
    "\n",
    "show_errors(x_test, model, y_test, int2word, n_samples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictioins with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = ['the film was really bad and i am very disappointed',\n",
    "           'The film was very funny entertaining and good we had a great time . brilliant film',\n",
    "           'this film was just brilliant',\n",
    "           'the movie is not bad',\n",
    "           'the movie is not bad I like it'\n",
    "]\n",
    "sequences = [vectorize_text_sentence(review.lower(), word2int)\n",
    "             for review in reviews]\n",
    "\n",
    "x_pred = vectorize_sequences(sequences, num_words=num_words)\n",
    "predictions = model.predict(x_pred)\n",
    "for review, pred in zip(reviews, predictions.flatten()):\n",
    "    print()\n",
    "    print(review)\n",
    "    print('Sentiment: ', np.round(pred, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.0*(model.predict(x_pred) > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Repeat the process with unnormalized bag of words, compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorize_sequences(train_data, num_words=num_words, norm=False)\n",
    "x_test = vectorize_sequences(test_data, num_words=num_words, norm=False)\n",
    "y_train =np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "history = model.fit(x_train, y_train, validation_split=0.25, epochs= 10, batch_size=32)\n",
    "show_loss_accuracy_evolution(hist)\n",
    "results = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: What happens if you reduce the size of the vocabulary `num_words`, compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = ...\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=num_words)\n",
    "print(train_data[0])\n",
    "\n",
    "word2int = imdb.get_word_index()\n",
    "word2int = {w: i+3 for w, i in word2int.items()}\n",
    "word2int[\"<PAD>\"] = 0\n",
    "word2int[\"<START>\"] = 1\n",
    "word2int[\"<UNK>\"] = 2\n",
    "word2int[\"<UNUSED>\"] = 3\n",
    "int2word = {i: w for w, i in word2int.items()}\n",
    "num_words = num_words+3\n",
    "\n",
    "x_train = vectorize_sequences(train_data, num_words=num_words)\n",
    "x_test = vectorize_sequences(test_data, num_words=num_words)\n",
    "y_train =np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "history = model.fit(x_train, y_train, validation_split=0.25, epochs= 10, batch_size=32)\n",
    "show_loss_accuracy_evolution(hist)\n",
    "results = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [\n",
    "    'the film was really bad and i am very disappointed',\n",
    "    'The film was very funny entertaining and good we had a great time . brilliant film',\n",
    "    'this film was just brilliant', 'the movie is not bad',\n",
    "    'the movie is not bad I like it'\n",
    "]\n",
    "sequences = [\n",
    "    vectorize_text_sentence(review.lower(), word2int) for review in reviews\n",
    "]\n",
    "\n",
    "x_pred = vectorize_sequences(sequences, num_words=num_words)\n",
    "predictions = model.predict(x_pred)\n",
    "for review, pred in zip(reviews, predictions.flatten()):\n",
    "    print()\n",
    "    print(review)\n",
    "    print('Sentiment: ', np.round(pred, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with other ML algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clr = LogisticRegression()\n",
    "clr.fit(x_train, y_train)\n",
    "val_acc = clr.score(x_test, y_test)\n",
    "print('Test Accuracy: {}'.format(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(max_depth=5, n_jobs=-1)\n",
    "rfc.fit(x_train, y_train)\n",
    "val_acc = rfc.score(x_test, y_test)\n",
    "print('Test Accuracy: {}'.format(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_jobs=-1)\n",
    "rfc.fit(x_train, y_train)\n",
    "val_acc = rfc.score(x_test, y_test)\n",
    "print('Test Accuracy: {}'.format(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = []\n",
    "# training reviews\n",
    "for ind in range(len(train_data)):\n",
    "    sentence = train_data[ind]\n",
    "    sentence_text = get_words(train_data[ind], int2word)\n",
    "    reviews.append(sentence_text)\n",
    "print('First training review: ', reviews[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
