{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f310246",
   "metadata": {},
   "source": [
    "It is highly recommended to use a powerful **GPU**, you can use it for free uploading this notebook to [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb).\n",
    "<table align=\"center\">\n",
    " <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/ezponda/intro_deep_learning/blob/main/class/Fundamentals/Regression_tuner.ipynb\">\n",
    "        <img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
    "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/ezponda/intro_deep_learning/blob/main/class/Fundamentals/Regression_tuner.ipynb\">\n",
    "        <img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />View Source on GitHub</a></td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2360c95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "tf.keras.utils.set_random_seed(0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45e3013",
   "metadata": {},
   "source": [
    "# Abalone Dataset\n",
    "\n",
    "Abalones are marine snails that can be found along coasts of almost every continent. \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/0b/AbaloneInside.jpg/440px-AbaloneInside.jpg\" alt=\"abalone\" border=\"0\" width=\"400\" height=\"500\">\n",
    "\n",
    "\n",
    "\n",
    "In this notebook we are going to Predict the age of abalone from physical measurements. [Link to documentation](https://archive.ics.uci.edu/ml/datasets/abalone)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801c6c84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\n",
    "    \"https://storage.googleapis.com/download.tensorflow.org/data/abalone_train.csv\",\n",
    "    names=[\"Length\", \"Diameter\", \"Height\", \"Whole weight\", \"Shucked weight\",\n",
    "           \"Viscera weight\", \"Shell weight\", \"Age\"])\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edcad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c554e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train.pop('Age')\n",
    "X_train = df_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fede61bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\n",
    "    \"https://storage.googleapis.com/download.tensorflow.org/data/abalone_test.csv\",\n",
    "    names=[\"Length\", \"Diameter\", \"Height\", \"Whole weight\", \"Shucked weight\",\n",
    "           \"Viscera weight\", \"Shell weight\", \"Age\"])\n",
    "y_test = df_test.pop('Age')\n",
    "X_test = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de841977",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'X_train shape: {X_train.shape}, X_test shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1bf8de",
   "metadata": {},
   "source": [
    "## Regression Losses\n",
    "\n",
    "- **Mean Squared Error (MSE)**: \n",
    "\n",
    "```python\n",
    "tf.keras.losses.MSE\n",
    "```\n",
    "```python\n",
    "model.compile(loss='mse') or model.compile(loss=tf.keras.losses.MSE)\n",
    "```\n",
    "\n",
    "$$ \\mathrm{MSE} = \\frac{\\sum_{i=1}^n\\left( y_i - \\hat{y_i}\\right)^2}{n}$$\n",
    "\n",
    "\n",
    "- **Mean Absolute Error (MAE)**: \n",
    "\n",
    "```python\n",
    "tf.keras.losses.MAE\n",
    "```\n",
    "```python\n",
    "model.compile(loss='mae') or model.compile(loss=tf.keras.losses.MAE)\n",
    "```\n",
    "\n",
    "$$ \\mathrm{MAE} = \\frac{\\sum_{i=1}^n\\left| y_i - \\hat{y_i}\\right|}{n}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313693e1",
   "metadata": {},
   "source": [
    "## Question 1: Create a net with at least 1 hidden layer\n",
    "\n",
    "\n",
    "1. You can use the [Functional API](https://keras.io/guides/functional_api/):\n",
    "\n",
    "You need to start with an input data entry:\n",
    "```python    \n",
    "    inputs = keras.Input(shape=(...,))\n",
    "    layer_1 = layers.Dense(...)(inputs)\n",
    "```\n",
    "\n",
    "and the network outputs:\n",
    "```python\n",
    "outputs = layers.Dense(...)(previous_layer)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "```\n",
    "\n",
    "2. Or you can use [Sequential API](https://keras.io/guides/sequential_model/)\n",
    "\n",
    "```python\n",
    "model = keras.Sequential(name='example_model')\n",
    "model.add(layers.Dense(..., input_shape=(8,))\n",
    "model.add(...\n",
    "```\n",
    "\n",
    "You can introduce regularization methods seen in [Prevent_Overfitting.ipynb](https://github.com/ezponda/intro_deep_learning/blob/main/class/Fundamentals/Prevent_Overfitting.ipynb) like [Dropout layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout):\n",
    "\n",
    "\n",
    "```python\n",
    "tf.keras.layers.Dropout(\n",
    "    rate, noise_shape=None, seed=None, **kwargs\n",
    ")\n",
    "```\n",
    "\n",
    "With Functional API:\n",
    "```python\n",
    "next_layer = layers.Dropout(0.4)(prev_layer)\n",
    "```\n",
    "With Sequential:\n",
    "```python\n",
    "model.add(layers.Dropout(0.4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f142132",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "model.add(layers.Dense(..., input_shape=(...,), activation=...))\n",
    "...\n",
    "# output layer\n",
    "model.add(layers.Dense(..., activation=...))\n",
    "\n",
    "## model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b73b78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Functional API\n",
    "# 1. Define the input layer\n",
    "inputs = keras.Input(shape=(...,))\n",
    "\n",
    "# 2. Build the hidden layers, You can also add dropout or other layer types\n",
    "x = layers.Dense(..., activation=...)(inputs)\n",
    "...\n",
    "x = layers.Dropout(...)(x)\n",
    "x = layers.Dense(..., activation=...)(x)\n",
    "\n",
    "# 3. Define the output layer\n",
    "outputs = layers.Dense(..., activation=...)(x)\n",
    "\n",
    "# 4. Create the model by specifying inputs and outputs\n",
    "model = keras.Model(inputs=..., outputs=..., name='functional_model')\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b42f777",
   "metadata": {},
   "source": [
    "[Early stopping callback](https://keras.io/api/callbacks/early_stopping/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceb6dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping callback \n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=...,\n",
    "    patience=...,\n",
    "    min_delta=...,\n",
    "    restore_best_weights=...,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bb1a7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=...,\n",
    "    metrics=[...],\n",
    ")\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    validation_split=0.2,\n",
    "    batch_size=32,\n",
    "    callbacks=...,  # early stopping\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdfd5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def show_loss_evolution(history):\n",
    "    \n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.plot(hist['epoch'], hist['loss'], label='Train Error')\n",
    "    plt.plot(hist['epoch'], hist['val_loss'], label='Val Error')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "show_loss_evolution(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644f80fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test Loss: {}'.format(results[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c909754",
   "metadata": {},
   "source": [
    "## Question 2: Normalize the inputs and train the same model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ed6000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_norm = ...\n",
    "X_test_norm = ...\n",
    "print('X_train mu, sigma', X_train_norm.mean(0), X_train_norm.std(0))\n",
    "print('X_test mu, sigma', X_test_norm.mean(0), X_test_norm.std(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ac44c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "...\n",
    "## model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1d2308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping callback\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=...,\n",
    "    patience=...,\n",
    "    min_delta=...,\n",
    "    restore_best_weights=...,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173bef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.MSE,\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train_norm,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    validation_split=0.2,\n",
    "    batch_size=32,\n",
    "    callbacks=...,  # early stoppings\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6040ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test_norm, y_test, verbose=1)\n",
    "print('Test Loss: {}'.format(results[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de862aa9",
   "metadata": {},
   "source": [
    "## Optimizers:\n",
    "\n",
    "- [SGD](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD): Gradient descent with momentum\n",
    "```python\n",
    "tf.keras.optimizers.SGD(\n",
    "    learning_rate=0.01, momentum=0.0, nesterov=False, name='SGD', **kwargs\n",
    ")\n",
    "```\n",
    "If momentum is 0:\n",
    "```python\n",
    "w = w - learning_rate * gradient\n",
    "```\n",
    "If we have momentum:\n",
    " \n",
    " ```python\n",
    "velocity = momentum * velocity - learning_rate * g\n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "\n",
    "- [RMSprop](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/RMSprop): Root Mean Square Propagation\n",
    "```python\n",
    "tf.keras.optimizers.RMSprop(\n",
    "    learning_rate=0.001, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False,\n",
    "    name='RMSprop', **kwargs\n",
    ")\n",
    "```\n",
    "- [Adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam): Adaptive Moment Estimation,  is an update to the RMSProp algorithm\n",
    "```python\n",
    "tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "    name='Adam', **kwargs\n",
    ")\n",
    "```\n",
    "\n",
    "```python\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d744232",
   "metadata": {},
   "source": [
    "## Question 3: Train the same model with different optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c797bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "...\n",
    "## model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81ef1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=...,\n",
    "    loss=...,\n",
    "    metrics=[...]\n",
    ")\n",
    "model.fit(X_train_norm, y_train, epochs=50, validation_split=0.2, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36eaf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test_norm, y_test, verbose=1)\n",
    "print('Test Loss: {}'.format(results[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2377a7ff",
   "metadata": {},
   "source": [
    "# Keras Tuner : Introduction to Hyperparameter Optimization\n",
    "\n",
    "The [Keras Tuner](https://www.tensorflow.org/tutorials/keras/keras_tuner) is a library for hyper-parameter tuning.\n",
    "\n",
    "## What is Hyperparameter Tuning?\n",
    "\n",
    "Hyperparameter tuning is the process of finding the optimal set of hyperparameters for a machine learning model to maximize its performance. Unlike model parameters (weights and biases) that are learned during training, hyperparameters are set before the learning process begins and influence how the model learns.\n",
    "\n",
    "Common hyperparameters include:\n",
    "- **Model architecture**: number of layers, units per layer\n",
    "- **Training parameters**: learning rate, batch size, dropout rate\n",
    "- **Regularization**: L1/L2 penalties, early stopping criteria\n",
    "\n",
    "Manual tuning of these parameters can be time-consuming and often leads to suboptimal results. Keras Tuner provides an automated approach to efficiently search the hyperparameter space.\n",
    "\n",
    "## Understanding Different Tuning Strategies\n",
    "\n",
    "Keras Tuner offers four main strategies for hyperparameter optimization:\n",
    "\n",
    "### 1. RandomSearch\n",
    "- **How it works**: Randomly samples from the hyperparameter space\n",
    "- **Pros**: Simple, easily parallelizable, no assumptions about parameter importance\n",
    "- **Cons**: Can be inefficient for large search spaces\n",
    "- **Best for**: Initial exploration or when little is known about the hyperparameter landscape\n",
    "\n",
    "### 2. Hyperband\n",
    "- **How it works**: Allocates resources (epochs) dynamically, quickly discarding poor performers\n",
    "- **Pros**: More efficient than random search, especially for deep networks\n",
    "- **Cons**: More complex to configure correctly\n",
    "- **Best for**: When training is computationally expensive and you want to balance exploration vs. exploitation\n",
    "\n",
    "### 3. BayesianOptimization\n",
    "- **How it works**: Builds a probability model of the objective function and uses it to select hyperparameters\n",
    "- **Pros**: More efficient use of resources, learns from previous evaluations\n",
    "- **Cons**: More complex, computationally intensive for each iteration\n",
    "- **Best for**: When evaluation is expensive and you have a moderate search space\n",
    "\n",
    "### 4. Sklearn\n",
    "- **How it works**: Interface to scikit-learn's hyperparameter search methods\n",
    "- **Pros**: Familiar API for those coming from scikit-learn\n",
    "- **Cons**: Limited to sklearn's hyperparameter tuning capabilities\n",
    "- **Best for**: When integrating with existing sklearn pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e7af43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f47d1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kerastuner as kt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e9b048",
   "metadata": {},
   "source": [
    "Hyperparameters are of two types:\n",
    "1. **Model hyperparameters** like number of units, type of activation or number hidden layers.\n",
    "2. **Algorithm hyperparameters** like the learning rate in adam.\n",
    "\n",
    "The model-building function takes an argument `hp` from which you can sample hyper-parameters.\n",
    "\n",
    "```python\n",
    "def build_model(hp):\n",
    "    ...\n",
    "    return model\n",
    "\n",
    "```\n",
    "\n",
    "- `hp.Int` to sample an integer from a certain range:\n",
    "```python\n",
    "hp.Int('units', min_value=32, max_value=256, step=32, default=64)\n",
    "```\n",
    "- `hp.Float` to sample a float number from a certain range:\n",
    "```python\n",
    "hp.Float('dropout', min_value=0.0, max_value=0.1, default=0.005, step=0.05)\n",
    "```\n",
    "- `hp.Choice` to select values in a list:\n",
    "```python\n",
    "hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
    "```\n",
    "- [list of hyperparameter methods](https://keras-team.github.io/keras-tuner/documentation/hyperparameters/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef99c2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    # Sample different number of layers with hp.Int\n",
    "    for i in range(hp.Int('num_layers', 1, 3)):\n",
    "        # Sample different number of layers with hp.Int\n",
    "        model.add(layers.Dense(units=hp.Int('units_' + str(i),\n",
    "                                            min_value=64,\n",
    "                                            max_value=128,\n",
    "                                            step=32),\n",
    "                               activation='relu'))\n",
    "    # Sample different activation functions with hp.Choice \n",
    "    model.add(layers.Dense(1, activation=hp.Choice('output_activation', ['relu', 'linear'])))\n",
    "    \n",
    "    # Sample different activation functions with hp.Choice \n",
    "    model.compile(\n",
    "        loss='mse',\n",
    "        metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71047eeb",
   "metadata": {},
   "source": [
    "The Keras Tuner has four [tuners](https://keras-team.github.io/keras-tuner/documentation/tuners/) available  `RandomSearch`, `Hyperband`, `BayesianOptimization`, and `Sklearn`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8261f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "tuner = kt.Hyperband(build_model,\n",
    "                     objective='val_loss',\n",
    "                     max_epochs=15,\n",
    "                     factor=3,\n",
    "                     hyperband_iterations=1,\n",
    "                     directory='my_dir',\n",
    "                     project_name='intro_to_kt')\n",
    "'''\n",
    "\n",
    "tuner = kt.RandomSearch(build_model,\n",
    "                     objective='val_loss',\n",
    "                     max_trials=20,\n",
    "                     directory='my_dir',\n",
    "                     project_name='intro_to_kt')\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322e674d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(X_train_norm, y_train, epochs=20, validation_split=0.15, batch_size=32, callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(best_hps.get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df6dcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d31a650",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best output activation function: {best_hps.get('output_activation')}\")\n",
    "print(f\"Best number of hidden layers: {best_hps.get('num_layers')}\")\n",
    "for i in range(best_hps.get('num_layers')):\n",
    "    print(f\"Number of units of hidden layer {i+1}: {best_hps.get('units_' + str(i))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bc610d",
   "metadata": {},
   "source": [
    "### Hyperparameter Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f740b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import seaborn as sns\n",
    "\n",
    "def analyze_hyperparameter_importance(tuner, top_n=10):\n",
    "    \"\"\"\n",
    "    Analyze the importance of different hyperparameters using Random Forest.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    tuner : keras_tuner.Tuner\n",
    "        The tuner object after running a hyperparameter search\n",
    "    top_n : int, default=10\n",
    "        Number of top trials to print\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    importance_df : pandas.DataFrame\n",
    "        DataFrame containing the importance of each hyperparameter\n",
    "    \"\"\"\n",
    "    print(\"Extracting hyperparameter data from tuner...\")\n",
    "    \n",
    "    # Extract hyperparameters and scores from tuner\n",
    "    hp_results = []\n",
    "    \n",
    "    for trial_id, trial in tuner.oracle.trials.items():\n",
    "        if trial.score is not None:  # Only include completed trials\n",
    "            # Get hyperparameters\n",
    "            hp_values = trial.hyperparameters.values\n",
    "            # Add score (lower is better for loss)\n",
    "            hp_values['score'] = trial.score\n",
    "            hp_results.append(hp_values)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(hp_results)\n",
    "    \n",
    "    # Show basic stats\n",
    "    print(f\"Analyzed {len(results_df)} trials\")\n",
    "    print(\"\\nTop {top_n} configurations:\".format(top_n=top_n))\n",
    "    top_configs = results_df.sort_values('score').head(top_n)\n",
    "    for i, (_, config) in enumerate(top_configs.iterrows()):\n",
    "        print(f\"\\nRank {i+1} (Score: {config['score']:.4f}):\")\n",
    "        for param, value in sorted(config.items()):\n",
    "            if param != 'score':\n",
    "                print(f\"  {param}: {value}\")\n",
    "    \n",
    "    # Prepare data for Random Forest\n",
    "    X = results_df.drop('score', axis=1)\n",
    "    \n",
    "    # Convert non-numeric columns to numeric\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == 'object':\n",
    "            # Create dummies for categorical variables\n",
    "            dummies = pd.get_dummies(X[col], prefix=col)\n",
    "            X = pd.concat([X.drop(col, axis=1), dummies], axis=1)\n",
    "    \n",
    "    y = results_df['score']\n",
    "    \n",
    "    # Train Random Forest for feature importance\n",
    "    print(\"\\nTraining Random Forest to analyze hyperparameter importance...\")\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf.fit(X, y)\n",
    "    \n",
    "    # Get feature importance\n",
    "    importance = rf.feature_importances_\n",
    "    \n",
    "    # Create DataFrame of features and importance\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': importance\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Importance', y='Feature', data=importance_df.head(15))\n",
    "    plt.title('Hyperparameter Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print importance values\n",
    "    print(\"\\nHyperparameter importance ranking:\")\n",
    "    for i, (_, row) in enumerate(importance_df.iterrows()):\n",
    "        if i < 15:  # Print top 15\n",
    "            print(f\"{i+1}. {row['Feature']}: {row['Importance']:.4f}\")\n",
    "    \n",
    "    # Analyze top feature's impact with partial dependence plot\n",
    "    top_feature = importance_df.iloc[0]['Feature']\n",
    "    if top_feature in X.columns:  # Ensure it's a numeric feature\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.scatter(X[top_feature], y, alpha=0.6)\n",
    "        plt.title(f'Impact of {top_feature} on Model Performance')\n",
    "        plt.xlabel(top_feature)\n",
    "        plt.ylabel('Score (lower is better)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"\\nAnalysis complete. Use these insights to refine your hyperparameter search!\")\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "\n",
    "\n",
    "# After running tuner.search():\n",
    "\n",
    "# 1. Get overall importance\n",
    "importance_df = analyze_hyperparameter_importance(tuner)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df9ffad",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b439536",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(X_train_norm, y_train, epochs=50, validation_split=0.15, callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57509bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test_norm, y_test, verbose=1)\n",
    "print('Test Loss: {}'.format(results[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbf5a27",
   "metadata": {},
   "source": [
    "## Question 4: Try to search with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c7b527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    \"\"\"Build a model with tunable architecture and dropout regularization.\"\"\"\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # Tune the number of layers (try 1-4 layers)\n",
    "    for i in range(hp.Int('num_layers', min_value=..., max_value=...)):\n",
    "        # Tune the number of units in each layer\n",
    "        model.add(layers.Dense(\n",
    "            units=hp.Int(f'units_{i}', min_value=..., max_value=..., step=...),\n",
    "            activation=hp.Choice(f'activation_{i}', values=[...])\n",
    "        ))\n",
    "        \n",
    "        # Add dropout after the dense layer\n",
    "        # Hint: Experiment with different dropout ranges\n",
    "        model.add(layers.Dropout(\n",
    "            hp...(..., min_value=..., max_value=..., step=...)\n",
    "        ))\n",
    "    '''\n",
    "    # Output layer for regression)\n",
    "    model.add(layers.Dense(1, activation=...))\n",
    "\n",
    "    # Tune the optimizer and learning rate\n",
    "    # Hint: Try different optimizers and log-scale for learning rate\n",
    "    optimizer_choice = hp.Choice('optimizer', values=[...])\n",
    "    learning_rate = hp.Float('learning_rate', min_value=..., max_value=..., sampling=...)\n",
    "    \n",
    "    if optimizer_choice == 'adam':\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == 'rmsprop':\n",
    "        optimizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = ...\n",
    "    '''\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=...,\n",
    "        metrics=[...]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659d9de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(build_model,\n",
    "                     objective='val_loss',\n",
    "                     max_epochs=15,\n",
    "                     factor=3,\n",
    "                     hyperband_iterations=1,\n",
    "                     directory='my_dir_2',\n",
    "                     project_name='intro_to_kt')\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=...,\n",
    "    patience=...,\n",
    "    )\n",
    "tuner.search(X_train_norm, y_train, epochs=20, validation_split=0.15,\n",
    "             batch_size=32, callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(best_hps.get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb414e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best learning rate: {best_hps.get('learning_rate')}\")\n",
    "print(f\"Best output activation function: {best_hps.get('output_activation')}\")\n",
    "print(f\"Best number of hidden layers: {best_hps.get('num_layers')}\")\n",
    "for i in range(best_hps.get('num_layers')):\n",
    "    print(f\"Number of units of hidden layer {i+1}: {best_hps.get('units_' + str(i))}\")\n",
    "    #print(f\"Dropout rate of hidden layer {i+1}: {best_hps.get('dp_' + str(i))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef4f3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(X_train_norm, y_train, epochs=50, validation_split=0.15, callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f46fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test_norm, y_test, verbose=1)\n",
    "print('Test Loss: {}'.format(results[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
