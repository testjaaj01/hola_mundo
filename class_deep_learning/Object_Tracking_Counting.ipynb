{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e982111e",
   "metadata": {},
   "source": [
    "It is highly recommended to use a powerful **GPU**, you can use it for free uploading this notebook to [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb).\n",
    "<table align=\"center\">\n",
    " <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/ezponda/intro_deep_learning/blob/main/class/CNN/Object_Tracking_Counting.ipynb\">\n",
    "        <img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
    "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/ezponda/intro_deep_learning/blob/main/class/CNN/Object_Tracking_Counting.ipynb\">\n",
    "        <img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />View Source on GitHub</a></td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290492d1",
   "metadata": {},
   "source": [
    "# Object Tracking with Ultralytics YOLO\n",
    "\n",
    "Object tracking extends the capabilities of object detection by not only identifying and localizing objects within each frame of a video but also maintaining a unique identifier for each detected object across the frames. This process is crucial for applications that require monitoring the movement and behavior of objects over time, such as surveillance, sports analytics, and advanced driver-assistance systems (ADAS).\n",
    "\n",
    "## Differences Between Object Tracking and Detection\n",
    "\n",
    "- **Object Detection** is the process of identifying objects within a single image frame and classifying them into predefined categories. It provides the location of the object typically in the form of bounding boxes.\n",
    "\n",
    "- **Object Tracking**, on the other hand, follows the identified objects from one frame to the next. This involves detecting the object in the initial frame and then tracking the objectâ€™s movement across subsequent frames, maintaining a consistent identifier for each object.\n",
    "\n",
    "## Features of Ultralytics YOLO Object Tracking\n",
    "\n",
    "Ultralytics YOLO offers enhanced object tracking capabilities:\n",
    "\n",
    "- **Real-Time Tracking**: Capable of tracking objects seamlessly in high-frame-rate videos, making it suitable for real-time applications.\n",
    "- **Multiple Tracker Support**: Provides options to select from various established tracking algorithms.\n",
    "- **Customizable Tracker Configurations**: Allows adjustment of tracker parameters to suit specific requirements, enhancing flexibility and performance.\n",
    "\n",
    "## Available Trackers in Ultralytics YOLO\n",
    "\n",
    "Ultralytics YOLO supports different tracking algorithms, including:\n",
    "\n",
    "- **BoT-SORT**: Default tracking algorithm, suitable for a variety of tracking needs. Enable with `tracker='botsort.yaml'`.\n",
    "- **ByteTrack**: Known for its high performance, especially in dense scenes. Enable with `tracker='bytetrack.yaml'`.\n",
    "\n",
    "## Implementing Object Tracking\n",
    "\n",
    "To apply object tracking with Ultralytics YOLO, you can use the following code snippets:\n",
    "\n",
    "### Loading and Running a Tracker\n",
    "\n",
    "```python\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a pretrained model\n",
    "model = YOLO('yolov8n.pt')  # Substitute with yolov8n-seg.pt or yolov8n-pose.pt as needed\n",
    "\n",
    "# Perform tracking on a video source\n",
    "results = model.track(source=\"path/to/video.mp4\", show=True, tracker=\"botsort.yaml\")  # Use \"bytetrack.yaml\" for ByteTrack\n",
    "```\n",
    "\n",
    "This code will run the specified tracker on the video source, displaying the tracked objects in real-time.\n",
    "\n",
    "### Configuration and Customization\n",
    "\n",
    "Tracking configuration can be customized similar to prediction settings, including confidence and IoU thresholds:\n",
    "\n",
    "```python\n",
    "# Configure the tracking parameters\n",
    "results = model.track(source=\"path/to/video.mp4\", conf=0.3, iou=0.5, show=True)\n",
    "```\n",
    "\n",
    "You can also use a custom tracker configuration by modifying a YAML file from the `ultralytics/cfg/trackers` directory:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fb291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchvision torchaudio\n",
    "#!pip install ultralytics\n",
    "#!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ada4e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set the environment variable within the notebook\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504c82c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2346a291",
   "metadata": {},
   "source": [
    "### Optional: Web cam Local (Not for Colab) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac66d0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import time\n",
    "from IPython.display import Image as IPyImage\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "time.sleep(1)  ### letting the camera autofocus\n",
    "\n",
    "axes = None\n",
    "NUM_FRAMES = 100  # you can change this\n",
    "processed_imgs = []\n",
    "for i in range(NUM_FRAMES):\n",
    "    # Load frame from the camera\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Run the model\n",
    "    results = model.track(frame, persist=True, verbose=False)\n",
    "    result = results[0]\n",
    "    \n",
    "    annotated_image_bgr = result.plot()\n",
    "    annotated_image_rgb = annotated_image_bgr[..., ::-1]  # Convert BGR to RGB\n",
    "    \n",
    "    img = Image.fromarray(np.uint8(annotated_image_rgb))\n",
    "    processed_imgs.append(img)\n",
    "    cv2.imshow(\"test\", annotated_image_bgr)\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e69ee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create gif\n",
    "processed_imgs[0].save('web_cam_tracking.gif',\n",
    "                       format='GIF',\n",
    "                       append_images=processed_imgs[1:],\n",
    "                       save_all=True,\n",
    "                       duration=100,\n",
    "                       loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06a5858",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IPyImage('web_cam_tracking.gif', format='png', width=15 * 40, height=3 * 40) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1f7854",
   "metadata": {},
   "source": [
    "### Traffic scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aaea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8n.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293a10de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_frame(img):\n",
    "    \"\"\"\n",
    "    Displays an image frame. Automatically detects the execution environment.\n",
    "    Args:\n",
    "        img (numpy.ndarray): The image frame to be displayed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from google.colab.patches import cv2_imshow\n",
    "        from IPython.display import clear_output\n",
    "        clear_output(wait=True)  # Clear the output to prevent image accumulation\n",
    "        cv2_imshow(img)  \n",
    "    except ImportError:\n",
    "        cv2.imshow(\"YOLOv8 Tracking\", img)\n",
    "        \n",
    "traffic_url_1 = \"https://github.com/ezponda/intro_deep_learning/raw/main/images/road_traffic_video_for_object_recognition_short.mp4\"\n",
    "\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(traffic_url_1)\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    \n",
    "    if success:\n",
    "        # Run YOLOv8 tracking on the frame\n",
    "        results = model.track(frame, persist=True, verbose=False)\n",
    "        result = results[0]\n",
    "\n",
    "        # Visualize and display the results\n",
    "        annotated_frame = result.plot()\n",
    "        show_frame(annotated_frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'): # Press 'q' to quit\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aee4d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3291e7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f444c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(result.boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e6bb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05de724",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Classes detected', result.boxes.cls.numpy())\n",
    "print('Total classes', result.names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54002237",
   "metadata": {},
   "source": [
    "## Question 1: Counting the Total Appearances of Trucks\n",
    "\n",
    "Count the total number of truck appearances across all frames in the video. Trucks are identified with the class index that appears in `result.names` . Your task is to complete the code such that it correctly counts the number of times trucks appear in the video frames. Take note of how the results are processed and how you can utilize the information contained within `result.boxes`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a9b4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Initialize the video\n",
    "cap = cv2.VideoCapture(traffic_url_1)\n",
    "truck_counts = ...  # Counter for truck appearances\n",
    "\n",
    "# Process video frames\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if success:\n",
    "        # Detect objects in the current frame\n",
    "        results = model.track(frame, persist=True, verbose=False)\n",
    "        result = results[0]\n",
    "        \n",
    "        # TODO: Complete the code to count truck appearances\n",
    "        ...\n",
    "        truck_counts += ...\n",
    "\n",
    "        # Display annotated frame\n",
    "        annotated_frame = result.plot()\n",
    "        show_frame(annotated_frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'): # Press 'q' to exit\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Cleanup and display result\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc01752",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total truck appearances: {truck_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6254e797",
   "metadata": {},
   "source": [
    "## Question 2: Counting Unique Trucks in the Video\n",
    "\n",
    "Modify the code to count the number of unique trucks that appear throughout the video. This involves tracking individual trucks across frames. Each detected truck has a unique tracking ID when tracking is enabled in YOLOv8. Utilize these IDs to identify unique trucks over the entire video. Implement the modifications necessary to count these unique instances.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d045761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Initialize the video\n",
    "cap = cv2.VideoCapture(traffic_url_1)\n",
    "unique_truck_ids = ...  # store unique truck IDs\n",
    "\n",
    "# Process video frames\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if success:\n",
    "        # Detect and track objects in the current frame\n",
    "        results = model.track(frame, persist=True, verbose=False)\n",
    "        result = results[0]\n",
    "\n",
    "        # TODO: Modify the code to track unique truck IDs\n",
    "        for i, cls_id in enumerate(result.boxes.cls.tolist()):\n",
    "            if cls_id == ...:  # Check if the class is a truck\n",
    "                # id: result.boxes.id[i].item()\n",
    "                ...\n",
    "\n",
    "        # Display annotated frame\n",
    "        annotated_frame = result.plot()\n",
    "        show_frame(annotated_frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'): # Press 'q' to exit\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Cleanup and display result\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7097896",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of unique trucks: {len(unique_truck_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dcfefb",
   "metadata": {},
   "source": [
    "## Counting Objects within a Region of Interest\n",
    "\n",
    "In this section, we explore how to focus our object detection efforts on specific areas of an image, known as Regions of Interest (ROI). Defining an ROI allows us to concentrate on particular parts of an image while ignoring irrelevant areas, making our object detection tasks more efficient and relevant.\n",
    "\n",
    "We will utilize a custom class, `ObjectCounter`, designed to count unique objects within the specified ROI. The ROI is defined by two points: the top-left corner `(x1, y1)` and the bottom-right corner `(x2, y2)`. These points outline a rectangular area in the image where our object detection and counting will be focused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dc15f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectCounter:\n",
    "    \"\"\"\n",
    "    A class for counting unique objects of specific classes within a specified region of interest (ROI) in video frames.\n",
    "    Now processes detection results directly.\n",
    "    \n",
    "\n",
    "    Attributes:\n",
    "        roi (tuple): The region of interest, defined as (x1, y1, x2, y2).\n",
    "        class_names (list): A list of class names to be counted.\n",
    "        unique_objects (dict): A dictionary to keep track of unique object IDs within the ROI, categorized by class.\n",
    "\n",
    "    Methods:\n",
    "        is_within_roi(box): Determines whether an object's bounding box is within the ROI.\n",
    "        update(result): Processes and updates counts based on the detection result.\n",
    "        get_count(): Returns the count of unique objects detected within the ROI, categorized by class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, roi, class_names=None):\n",
    "        \"\"\"\n",
    "        Initializes the ObjectCounter with a region of interest.\n",
    "\n",
    "        Args:\n",
    "            roi (tuple): The region of interest, defined as (x1, y1, x2, y2).\n",
    "            class_names (list or None): Specific class names to count within the ROI.\n",
    "        \"\"\"\n",
    "        self.roi = roi\n",
    "        self.unique_objects = {}\n",
    "        self.class_names = class_names\n",
    "\n",
    "    def is_within_roi(self, box):\n",
    "        \"\"\"\n",
    "        Determines whether the center of an object's bounding box is within the region of interest.\n",
    "        \n",
    "        - (x1, y1) are the coordinates of the top-left corner of a region within the image. Here, x1 corresponds to the column index, and y1 corresponds to the row index.\n",
    "        - (x2, y2) are the coordinates of the bottom-right corner of the region. Similarly, x2 corresponds to the column index, and y2 corresponds to the row index.\n",
    "\n",
    "        Args:\n",
    "            box (list or numpy.ndarray): The bounding box of an object, in the format [x1, y1, x2, y2].\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the object is within the ROI, False otherwise.\n",
    "        \"\"\"\n",
    "        x_center = (box[0] + box[2]) / 2\n",
    "        y_center = (box[1] + box[3]) / 2\n",
    "        return self.roi[0] < x_center < self.roi[2] and self.roi[1] < y_center < self.roi[3]\n",
    "\n",
    "    def update(self, result):\n",
    "        \"\"\"\n",
    "        Processes the detection result and updates the counts of unique objects within the ROI.\n",
    "\n",
    "        Args:\n",
    "            result (ultralytics.YOLOv5.detect.Detections): The detection results from YOLOv5.\n",
    "        \"\"\"\n",
    "        boxes = [box.xyxy[0].cpu().numpy().flatten() for box in result.boxes]\n",
    "        ids = [box.id.item() for box in result.boxes]\n",
    "        classes = [result.names[box.cls.item()] for box in result.boxes]\n",
    "\n",
    "        for box, obj_id, cls_name in zip(boxes, ids, classes):\n",
    "            if self.is_within_roi(box):\n",
    "                # Ensure we're tracking this class, and update the counter if so\n",
    "                if self.class_names is None or cls_name in self.class_names:\n",
    "                    if cls_name not in self.unique_objects:\n",
    "                        self.unique_objects[cls_name] = set()\n",
    "                    self.unique_objects[cls_name].add(obj_id)\n",
    "\n",
    "    def get_count(self):\n",
    "        \"\"\"\n",
    "        Returns the count of unique objects detected within the ROI, categorized by class.\n",
    "\n",
    "        Returns:\n",
    "            dict: The counts of unique objects within the ROI, categorized by class.\n",
    "        \"\"\"\n",
    "        return {cls: len(ids) for cls, ids in self.unique_objects.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea7cab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2  # Make sure OpenCV is installed and imported\n",
    "\n",
    "def show_frame(img, roi=None):\n",
    "    \"\"\"\n",
    "    Displays an image frame with an optional ROI. Automatically detects the execution environment.\n",
    "    \n",
    "    Args:\n",
    "        img (numpy.ndarray): The image frame to be displayed.\n",
    "        roi (tuple, optional): The region of interest defined as (x1, y1, x2, y2). If None, no ROI is drawn.\n",
    "    \"\"\"\n",
    "    # Draw the ROI on the frame if specified\n",
    "    if roi is not None:\n",
    "        x1, y1, x2, y2 = roi\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 3)  # Draw rectangle with green line\n",
    "\n",
    "    try:\n",
    "        from google.colab.patches import cv2_imshow\n",
    "        from IPython.display import clear_output\n",
    "        clear_output(wait=True)  # Clear the output to prevent image accumulation\n",
    "        cv2_imshow(img)  \n",
    "    except ImportError:\n",
    "        cv2.imshow(\"Frame with ROI\", img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e2d53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the region of interest and classes of interest\n",
    "roi = (100, 100, 400, 400)  # Adjust based on your video content\n",
    "class_names = ['truck', 'car', 'bicycle']  # Count these classes. Set to None to count all classes.\n",
    "\n",
    "object_counter = ObjectCounter(roi, class_names)\n",
    "\n",
    "cap = cv2.VideoCapture(traffic_url_1)\n",
    "\n",
    "# Process video frames\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if success:\n",
    "        # Detect and track objects in the current frame\n",
    "        results = model.track(frame, persist=True, verbose=False)\n",
    "        result = results[0]\n",
    "\n",
    "        # Update the object counter with detected objects\n",
    "        object_counter.update(result)\n",
    "\n",
    "        # Display annotated frame (this part remains unchanged)\n",
    "        annotated_frame = result.plot()\n",
    "        show_frame(annotated_frame, roi)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to exit\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2812b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique objects within ROI:\", object_counter.get_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40d4e14",
   "metadata": {},
   "source": [
    "## Question 4: Calculating the Lane-wise Difference in Truck Counts\n",
    "\n",
    "In this exercise, you will modify the object tracking system to differentiate between trucks passing through two distinct lanes of a road: the left lane and the right lane. You will define two Regions of Interest (ROIs), one for each lane, and count the number of unique trucks that pass through each. Finally, you will calculate the difference between the number of trucks passing through the right lane and the left lane.\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "1. Define two separate ROIs: one for the left lane and one for the right lane. These should be based on your video's specific lane positions.\n",
    "2. Use two instances of the `ObjectCounter` class, one for each lane, to count the number of trucks passing through.\n",
    "3. After processing the video, calculate the difference between the counts of unique trucks in the right lane and the left lane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62f168c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.imshow(annotated_frame[:,:,::-1])  # Display the image in RGB format\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e34a932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ROIs for the left and right lanes\n",
    "left_lane_roi = ... # (x1_left, y1_left, x2_left, y2_left)\n",
    "right_lane_roi = ... #(x1_right, y1_right, x2_right, y2_right)\n",
    "\n",
    "# Initialize the ObjectCounter for each lane\n",
    "left_lane_counter = ...\n",
    "right_lane_counter = ObjectCounter(..., class_names=[...])\n",
    "\n",
    "# Process video frames\n",
    "cap = cv2.VideoCapture(traffic_url_1)  # Replace 'traffic_url_1' with your video file path\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if success:\n",
    "        \n",
    "        # Detect and track objects in the current frame\n",
    "        ...\n",
    "\n",
    "        # Display annotated frame with the ROIs\n",
    "        annotated_frame = result.plot()\n",
    "        show_frame(annotated_frame, left_lane_roi)\n",
    "        show_frame(annotated_frame, right_lane_roi)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to exit\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe16f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print the difference in truck counts between the lanes\n",
    "left_lane_trucks = left_lane_counter.get_count().get('truck', 0)\n",
    "right_lane_trucks = right_lane_counter.get_count().get('truck', 0)\n",
    "difference = right_lane_trucks - left_lane_trucks\n",
    "print(f\"Truck count difference (Right lane - Left lane): {difference}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38282a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_lane_trucks, right_lane_trucks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcea69b",
   "metadata": {},
   "source": [
    "## Extra: [Workouts Monitoring using Ultralytics](https://docs.ultralytics.com/guides/workouts-monitoring/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea99c05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from ultralytics.solutions import ai_gym\n",
    "import cv2\n",
    "\n",
    "model = YOLO(\"yolov8n-pose.pt\")\n",
    "cap = cv2.VideoCapture(\"path/to/video\")\n",
    "assert cap.isOpened(), \"Error reading video file\"\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "\n",
    "video_writer = cv2.VideoWriter(\"workouts.avi\",\n",
    "                                cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "                                fps,\n",
    "                                (w, h))\n",
    "\n",
    "gym_object = ai_gym.AIGym()  # init AI GYM module\n",
    "gym_object.set_args(line_thickness=2,\n",
    "                    view_img=True,\n",
    "                    pose_type=\"pushup\",\n",
    "                    kpts_to_check=[6, 8, 10])\n",
    "\n",
    "frame_count = 0\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "      print(\"Video frame is empty or video processing has been successfully completed.\")\n",
    "      break\n",
    "    frame_count += 1\n",
    "    results = model.predict(frame, verbose=False)\n",
    "    frame = gym_object.start_counting(frame, results, frame_count)\n",
    "    cv2.imshow(\"Push-up Detection\", frame)\n",
    "    video_writer.write(frame)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "video_writer.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
