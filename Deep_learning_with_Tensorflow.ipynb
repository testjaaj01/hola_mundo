{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57c4867b",
   "metadata": {},
   "source": [
    "<table align=\"center\">\n",
    " <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/ezponda/intro_deep_learning/blob/main/Deep_learning_with_Tensorflow.ipynb\">\n",
    "        <img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
    "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/ezponda/intro_deep_learning/blob/main/Deep_learning_with_Tensorflow.ipynb\">\n",
    "        <img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />View Source on GitHub</a></td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b36a00a",
   "metadata": {},
   "source": [
    "**Table of Contents**\n",
    "\n",
    "- [Create a model in Tensorflow](#create-model)\n",
    "    - [Sequential API](#create-model-sequential)\n",
    "    - [Functional API](#create-model-functional)\n",
    "    - [Model Compilation](#create-model-compile)\n",
    "    - [Model Training](#create-model-fit)\n",
    "    - [Model Evaluation](#create-model-evaluating)\n",
    "- [Activation Functions](#activation-functions)\n",
    "- [Loss Functions](#loss-functions)\n",
    "- [Metrics](#metrics)\n",
    "- [Metrics vs Loss Functions](#metrics-vs-loss-functions)\n",
    "- [Regularization Techniques in Deep Learning](#regularization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3201494e",
   "metadata": {},
   "source": [
    "\n",
    "<a id='create-model'></a>\n",
    "## Create a model in Tensorflow\n",
    "\n",
    "<a id='create-model-sequential'></a>\n",
    "### Sequential API\n",
    "\n",
    "The Sequential API is a straightforward way to create models in Keras. It allows you to build a model layer by layer. Each layer has exactly one input tensor and one output tensor. It is called 'Sequential' because it allows you to sequentially stack layers in a neural network.\n",
    "\n",
    "The Sequential API is easy to use because of its simplicity, but it's not suitable for creating more complex models. For example, it's not straightforward to create models that have multiple inputs or outputs, share layers, or have non-linear topology (like reusing the output from an earlier layer as the input for a later layer) using the Sequential API.\n",
    "\n",
    "Here's an example of how to create a model using the Sequential API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e0e8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(10,)))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0675e5c",
   "metadata": {},
   "source": [
    "In this example, we first initialize a Sequential model. We then add a dense layer with 32 units and 'relu' activation function. The input_shape parameter is used to specify the shape of the input. The second layer is a Dense layer with 1 unit and a 'sigmoid' activation function.\n",
    "\n",
    "<a id='create-model-functional'></a>\n",
    "### Functional API\n",
    "\n",
    "The Functional API is a way to create models that are more complex than those that can be made with the Sequential API. It can handle models with multiple inputs or outputs, shared layers (the same layer called several times), and even non-linear topology (such as residual connections).\n",
    "\n",
    "With Functional API, you work directly with the tensors and you can manipulate them as you wish. This is why it's called 'Functional' – it takes the input tensor and output tensor to define a model.\n",
    "\n",
    "Here's an example of how to create a model using the Functional API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089c68c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# This returns a tensor\n",
    "inputs = Input(shape=(20,))\n",
    "\n",
    "# A layer instance is callable on a tensor and returns a tensor\n",
    "dense_layer = Dense(64, activation='relu')(inputs)\n",
    "outputs = Dense(10, activation='softmax')(dense_layer)\n",
    "\n",
    "# This creates a model that includes the Input layer and Dense layer\n",
    "model = Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e148389",
   "metadata": {},
   "source": [
    "\n",
    "In this example, we first create an input tensor using Input() function. We then create a Dense layer that takes the input tensor as input. We do this again to create another Dense layer. Finally, we create our model by specifying the inputs and outputs in the Model function.\n",
    "\n",
    "In summary, while the Sequential API is easier to use due to its simplicity, it's not suitable for creating complex models. The Functional API, on the other hand, is capable of creating complex models, but it requires a bit more work and understanding of how tensors work.\n",
    "\n",
    "<a id='create-model-compile'></a>\n",
    "### Model Compilation\n",
    "The `compile` method is used to configure the learning process before training the model. It receives three important arguments:\n",
    "\n",
    "- **Optimizer**: This is the algorithm that will be used to adjust the weights of the network to minimize the loss function. There are several optimizers available such as SGD, Adam, RMSProp, etc. They are usually specified by their string ID ('adam', 'sgd', etc.), but you can also pass an instance of the Optimizer class, which allows more customization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2372ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using string ID\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Using instance for more customization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "model.compile(optimizer=Adam(learning_rate=0.01), loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1966bbcd",
   "metadata": {},
   "source": [
    "- **Loss function**: This is the objective that the model will try to minimize. It can be the string identifier of an existing loss function (like 'categorical_crossentropy' or 'mse'), or it can be an objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3adbe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For binary classification\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# For multi-class classification\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# For regression\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a768a23f",
   "metadata": {},
   "source": [
    "- **Metrics**: These are the list of metrics to be evaluated during training and testing. Typically, you will use metrics=['accuracy']. To specify multiple metrics, pass them as a list: metrics=['accuracy', 'precision'].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1dab34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c13b692",
   "metadata": {},
   "source": [
    "<a id='create-model-fit'></a>\n",
    "### Model Training\n",
    "The `fit` method does the training loop: forward pass, backward pass and weight update. It has several arguments:\n",
    "\n",
    "- **x**: Input data. It could be a Numpy array (or array-like), a TensorFlow tensor, a dict mapping input names to the corresponding array/tensors, if the model has named inputs.\n",
    "\n",
    "- **y**: Target data. Like the input data x, it could be a Numpy array (or array-like), a TensorFlow tensor, a dict mapping output names to the corresponding array/tensors, if the model has named outputs.\n",
    "\n",
    "- **batch_size**: Number of samples per gradient update. If unspecified, batch_size will default to 32.\n",
    "\n",
    "```python\n",
    "model.fit(X_train, y_train, batch_size=64)\n",
    "```\n",
    "\n",
    "- **epochs**: Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided.\n",
    "\n",
    "```python\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=10)\n",
    "```\n",
    "\n",
    "- **validation_split / validation_data**: Fraction of the training data to be used as validation data / Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data.\n",
    "\n",
    "```python\n",
    "# Using validation_split\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=10, verbose=1, validation_split=0.2)\n",
    "\n",
    "# Using validation_data\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=10, verbose=1, validation_data=(X_val, y_val))\n",
    "```\n",
    "\n",
    "<a id='create-model-evaluating'></a>\n",
    "### Model Evaluation\n",
    "After training, you can evaluate the performance of the model on the test data:\n",
    "\n",
    "```python\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e78f330",
   "metadata": {},
   "source": [
    "<a id='activation-functions'></a>\n",
    "## Activation Functions\n",
    "\n",
    "Activation functions are mathematical equations that determine the output of a neural network. The function is attached to each neuron in the network, and determines whether it should be activated (“fired”) or not, based on whether each neuron’s input is relevant for the model’s prediction.\n",
    "\n",
    "### Sigmoid Function `activation='sigmoid'`\n",
    "\n",
    "The sigmoid function maps any value into a range between 0 and 1. It is useful in the output layer of a binary classification neural network, as it can be treated as a probability for the positive class.\n",
    "\n",
    "\n",
    "Sigmoid function is often used in the output layer of a binary classification where we need probabilities to classify data into two categories.\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "$$\n",
    "\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbcc496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    sx = sigmoid(x)\n",
    "    return sx * (1 - sx)\n",
    "\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y = sigmoid(x)\n",
    "dy = sigmoid_derivative(x)\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.plot(x, y, label='sigmoid')\n",
    "plt.plot(x, dy, label='derivative')\n",
    "plt.title('Sigmoid Activation Function')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b28994",
   "metadata": {},
   "source": [
    "### Hyperbolic Tangent function `activation='tanh'`\n",
    "\n",
    "The tanh function maps any value into a range between -1 and 1. It is useful in hidden layers of a neural network as it can create strong negative, neutral, and positive activations.\n",
    "\n",
    "\n",
    "Tanh is generally used in the hidden layers of a neural network as it can create strong negative, neutral, and positive activations.\n",
    "\n",
    "$$\n",
    "\\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tanh'(x) = 1 - \\tanh^{2}(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a0b13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - tanh(x)**2\n",
    "\n",
    "y = tanh(x)\n",
    "dy = tanh_derivative(x)\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.plot(x, y, label='tanh')\n",
    "plt.plot(x, dy, label='derivative')\n",
    "plt.title('Hyperbolic Tangent Activation Function')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dbe57b",
   "metadata": {},
   "source": [
    "### Rectified Linear Unit (ReLU) `activation='relu'`\n",
    "\n",
    "\n",
    "The ReLU function allows positive values to pass through directly, but clips negative values to zero. It is the most used activation function in convolutional neural networks (CNNs), where negative activations generally do not help with detecting features in an image.\n",
    "\n",
    "\n",
    "ReLU is often used in the hidden layers of Neural Networks, especially in Convolutional Neural Networks.\n",
    "\n",
    "\n",
    "$$\n",
    "f(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "$$\n",
    "f'(x) = \n",
    "  \\begin{cases} \n",
    "    1 & \\text{if } x > 0 \\\\\n",
    "    0 & \\text{otherwise} \n",
    "  \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc1da4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x <= 0, 0, 1)\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = relu(x)\n",
    "dy = relu_derivative(x)\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.plot(x, y, label='relu')\n",
    "plt.plot(x, dy, label='derivative')\n",
    "plt.title('ReLU Activation Function')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5929e288",
   "metadata": {},
   "source": [
    "###  Linear  `activation='linear'`\n",
    "\n",
    "\n",
    "The ReLU function allows positive values to pass through directly, but clips negative values to zero. It is the most used activation function in convolutional neural networks (CNNs), where negative activations generally do not help with detecting features in an image.\n",
    "\n",
    "\n",
    "ReLU is often used in the hidden layers of Neural Networks, especially in Convolutional Neural Networks.\n",
    "\n",
    "\n",
    "$$\n",
    "f(x) = x\n",
    "$$\n",
    "\n",
    "$$\n",
    "f'(x) = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718f5b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def linear_derivative(x):\n",
    "    return np.ones(x.shape)\n",
    "\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y = linear(x)\n",
    "dy = linear_derivative(x)\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.plot(x, y, label='linear')\n",
    "plt.plot(x, dy, label='derivative')\n",
    "plt.title('Linear Activation Function')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8372df5",
   "metadata": {},
   "source": [
    "### Softmax `activation='softmax'`\n",
    "\n",
    "\n",
    "The softmax function is used for multiclass classification problems. It gives the probability distribution of the event over 'n' different events. In other words, it calculates the probabilities of each target class over all possible target classes.\n",
    "\n",
    "The softmax function is generally used in the output layer of a classifier where we need to classify data into multiple categories.\n",
    "\n",
    "$$\n",
    "\\text{softmax}(x_{i}) = \\frac{e^{x_{i}}}{\\sum_{j=1}^{K} e^{x_{j}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6959b65",
   "metadata": {},
   "source": [
    "<a id='loss-functions'></a>\n",
    "## Loss Functions\n",
    "\n",
    "Loss functions, also known as cost functions, are used to evaluate how well specific algorithm models the given data. If predictions deviate too much from actual results, loss function would cough up a very large number. Gradually, with the help of some optimization function, loss function learns to reduce the error in prediction.\n",
    "\n",
    "### Regression Loss Functions\n",
    "\n",
    "#### Mean Squared Error Loss\n",
    "Mean Squared Error (MSE) is calculated by taking the average squared difference between the predicted and actual values. It's mainly used for regression problems.\n",
    "\n",
    "\n",
    "$$\n",
    "L(y, \\hat{y}) = \\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat{y}_{i})^{2}\n",
    "$$\n",
    "\n",
    "In TensorFlow, you can use MSE as your loss function like this:\n",
    "\n",
    "```python\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "````\n",
    "\n",
    "#### Mean Absolute Error Loss\n",
    "Mean Absolute Error (MAE) is calculated as the average of the absolute difference between the actual and predicted values.\n",
    "\n",
    "\n",
    "$$\n",
    "L(y, \\hat{y}) = \\frac{1}{n}\\sum_{i=1}^{n}|y_{i} - \\hat{y}_{i}|\n",
    "$$\n",
    "\n",
    "In TensorFlow, you can use MAE as your loss function like this:\n",
    "\n",
    "```python\n",
    "model.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "````\n",
    "\n",
    "### Classification Loss Functions\n",
    "\n",
    "#### Binary Cross-Entropy\n",
    "Binary Cross-Entropy loss function is used for binary classification problems. It's defined as the negative log-likelihood of the true labels given a probabilistic classifier’s predictions.\n",
    "\n",
    "\n",
    "$$\n",
    "L(y, \\hat{y}) = -\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}\\log(\\hat{y}_{i}) + (1 - y_{i})\\log(1 - \\hat{y}_{i})]\n",
    "$$\n",
    "In TensorFlow, you can use Binary Cross-Entropy as your loss function like this:\n",
    "\n",
    "```python\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "```\n",
    "\n",
    "#### Categorical Cross-Entropy\n",
    "Categorical Cross-Entropy is used for multi-class classification problems. It's defined as the negative log-likelihood of the true labels given a probabilistic classifier’s predictions.\n",
    "\n",
    "$$\n",
    "L(y, \\hat{y}) = -\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{j=1}^{K}y_{ij}\\log(\\hat{y}_{ij})\n",
    "$$\n",
    "\n",
    "In TensorFlow, you can use Categorical Cross-Entropy as your loss function like this:\n",
    "\n",
    "```python\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "```\n",
    "\n",
    "- **Categorical Cross-Entropy**\n",
    "Categorical Cross-Entropy is used when the labels are provided as a one-hot encoded vector (i.e., an array where the index representing the class is set to 1 and the rest are set to 0). This is often the case when you have more than two classes and the classes are mutually exclusive.\n",
    "\n",
    "For example, if you have three classes, A, B, and C, one-hot encoding of class A would be [1, 0, 0], class B would be [0, 1, 0] and class C would be [0, 0, 1].\n",
    "\n",
    "\n",
    "```python\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "````\n",
    "\n",
    "- **Sparse Categorical Cross-Entropy**\n",
    "Sparse Categorical Cross-Entropy is used when the labels are provided as integers, i.e., a single integer for each sample in the batch. This can be useful when dealing with large multi-class classification tasks with a lot of classes, as it saves memory by not having to store each class in a large one-hot encoded array.\n",
    "\n",
    "For example, instead of one-hot encoding as described above, class A could be represented as 0, class B as 1, and class C as 2.\n",
    "\n",
    "\n",
    "```python\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "```\n",
    "\n",
    "In a nutshell, the choice between using Categorical Cross-Entropy and Sparse Categorical Cross-Entropy depends on the format of your labels. If you have one-hot encoded labels, use Categorical Cross-Entropy. If you have integer labels, use Sparse Categorical Cross-Entropy.\n",
    "\n",
    "\n",
    "<a id='metrics'></a>\n",
    "## Metrics\n",
    "\n",
    "Metrics are functions used to evaluate the performance of your model. The choice of your metric function should align with the goal of your model.\n",
    "\n",
    "### Regression Metrics\n",
    "Mean Absolute Error (MAE)\n",
    "Mean Absolute Error (MAE) is the average of the absolute differences between the actual and predicted values.\n",
    "\n",
    "$$\n",
    "MAE = \\frac{1}{n}\\sum_{i=1}^{n}|y_{i} - \\hat{y}_{i}|\n",
    "$$\n",
    "\n",
    "In TensorFlow, you can use MAE as your metric like this:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "Mean Squared Error (MSE)\n",
    "Mean Squared Error (MSE) is the average of the squared differences between the actual and predicted values.\n",
    "\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat{y}_{i})^{2}\n",
    "$$\n",
    "In TensorFlow, you can use MSE as your metric like this:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "\n",
    "### Classification Metrics\n",
    "- **Accuracy**\n",
    "Accuracy is one of the most common classification metrics. It is defined as the proportion of correct predictions with respect to the targets.\n",
    "\n",
    "$$\n",
    "Accuracy = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\n",
    "$$\n",
    "In TensorFlow, you can use accuracy as your metric like this:\n",
    "\n",
    "```python\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    "- **Precision**\n",
    "Precision is defined as the proportion of true positive predictions (i.e., the number of items correctly identified as positive out of total predicted positives).\n",
    "\n",
    "$$\n",
    "Precision = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
    "$$\n",
    "In TensorFlow, you can use precision as your metric like this:\n",
    "\n",
    "```python\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['precision'])\n",
    "```\n",
    "\n",
    "- **Recall**\n",
    "Recall (also known as sensitivity) is defined as the proportion of actual positives that are correctly identified.\n",
    "\n",
    "\n",
    "$$\n",
    "Recall = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
    "$$\n",
    "In TensorFlow, you can use recall as your metric like this:\n",
    "\n",
    "```python\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['recall'])\n",
    "```\n",
    "\n",
    "Please note that precision and recall are more informative than accuracy when dealing with imbalanced datasets.\n",
    "\n",
    "<a id='metrics-vs-loss-functions'></a>\n",
    "## Metrics vs Loss Functions\n",
    "In machine learning, both metrics and loss functions are used to quantify the performance of the model. However, they serve different purposes and are used in different contexts.\n",
    "\n",
    "- **Loss Functions**: A loss function, also known as cost function, is used to optimize the machine learning model during training. It measures the inconsistency between predicted and actual values and the model tries to minimize this during training. It is this function that gets minimized by the optimization algorithm (such as Gradient Descent). For example, in a regression problem, Mean Squared Error (MSE) is often used as the loss function, while in a classification problem, Cross-Entropy is commonly used.\n",
    "\n",
    "- **Metrics**: Metrics, on the other hand, are used to evaluate the performance of the model. These are the key performance indicators that you care about and want to track during training and testing but you don't necessarily optimize for these during training. For example, in a classification problem, you might care about accuracy, precision, recall, or F1-score, while in a regression problem, you might care about Mean Absolute Error (MAE) or Root Mean Squared Error (RMSE).\n",
    "\n",
    "### When to Use Metrics\n",
    "\n",
    "Metrics are used when you want to monitor the performance of your model in terms that are understandable and interpretable in the context of the problem. They are not used to train the model but rather to gauge its performance on some validation data.\n",
    "\n",
    "For example, if you are dealing with a binary classification problem, your loss function could be 'binary_crossentropy', but your metric could be 'accuracy', because while cross-entropy is a good measure for the model to optimize over, accuracy (the ratio of correct predictions to total predictions) is more interpretable for us.\n",
    "\n",
    "In Keras, you can specify the metrics to use as a list of strings in the compile function:\n",
    "\n",
    "```python\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['recall', 'accuracy'])\n",
    "```\n",
    "\n",
    "In summary, metrics are used to understand the performance of the model during and after the training process, while loss functions are used by the optimization algorithm to train the model. It's important to choose both according to the problem at hand for effective model training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925cb7df",
   "metadata": {},
   "source": [
    "<a id='regularization'></a>\n",
    "## Regularization Techniques in Deep Learning\n",
    "\n",
    "Regularization is a technique used to prevent overfitting in a model. Overfitting occurs when the model performs well on the training data but poorly on the unseen data (like validation or test data). Regularization adds a penalty on the different parameters of the model to reduce the freedom of the model and in other words to avoid overfitting.\n",
    "\n",
    "### L1 and L2 Regularization\n",
    "\n",
    "L1 and L2 are the most common types of regularization. These update the general cost function by adding another term known as the regularization term.\n",
    "\n",
    "- L1 Regularization (Lasso Regression)\n",
    "In L1, we shrink the parameters to zero, thus removing some features and making the model simpler and interpretable.\n",
    "\n",
    "Here's the LaTeX code for L1 Regularization:\n",
    "\n",
    "$$\n",
    "L1 = \\lambda \\sum_{i=1}^{n} |w_{i}|\n",
    "$$\n",
    "In TensorFlow, you can add L1 regularization to your model like this:\n",
    "\n",
    "```python\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "model.add(Dense(64, input_dim=64,\n",
    "                kernel_regularizer=regularizers.l1(0.01),\n",
    "                activity_regularizer=regularizers.l1(0.01)))\n",
    "````\n",
    "\n",
    "- L2 Regularization (Ridge Regression)\n",
    "In L2, we shrink all the parameter weights to small and close to zero values, but it does not make them zero.\n",
    "\n",
    "$$\n",
    "L2 = \\lambda \\sum_{i=1}^{n} w_{i}^{2}\n",
    "$$\n",
    "In TensorFlow, you can add L2 regularization to your model like this:\n",
    "\n",
    "```python\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "model.add(Dense(64, input_dim=64,\n",
    "                kernel_regularizer=regularizers.l2(0.01),\n",
    "                activity_regularizer=regularizers.l2(0.01)))\n",
    "```\n",
    "### Dropout\n",
    "Dropout is a technique where randomly selected neurons are ignored during training, meaning that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass. It has the effect of reducing overfitting and improving model performance.\n",
    "\n",
    "In TensorFlow, you can add dropout to your model like this:\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "```\n",
    "In this case, approximately half (0.5) of the outputs from the previous layer will be dropped during training.\n",
    "\n",
    "### Early Stopping\n",
    "Early stopping is a form of regularization used to avoid overfitting when training a learner with an iterative method, such as gradient descent. These methods update the learner so as to make it better fit the training data with each iteration. Up to a point, this improves the learner's performance on data outside of the training set. Past that point, however, improving the learner's fit to the training data comes at the expense of increased generalization error.\n",
    "\n",
    "In TensorFlow, you can apply early stopping to your model like this:\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# And add early_stop to the list of callbacks in model.fit\n",
    "model.fit(X_train, y_train, validation_split=0.2, callbacks=[early_stop])\n",
    "```\n",
    "In this case, training will stop when the validation loss has not improved after 5 epochs.\n",
    "\n",
    "\n",
    "\n",
    "### Batch Normalization\n",
    "Batch Normalization is a technique that helps to make artificial neural networks faster and more stable through normalization of the layers' inputs by re-centering and re-scaling. It has been shown to have several benefits:\n",
    "\n",
    "\n",
    "The idea behind Batch Normalization is to normalize the outputs of each layer for each training mini-batch.\n",
    "\n",
    "$$\n",
    "\\mu_{B} = \\frac{1}{m}\\sum_{i=1}^{m}x_{i} % Mini-batch mean\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\sigma_{B}^{2} = \\frac{1}{m}\\sum_{i=1}^{m}(x_{i} - \\mu_{B})^{2} % Mini-batch variance\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\hat{x}_{i} = \\frac{x_{i} - \\mu_{B}}{\\sqrt{\\sigma_{B}^{2} + \\varepsilon}} % Normalize\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "y_{i} = \\gamma\\hat{x}_{i} + \\beta = BN_{\\gamma,\\beta}(x_{i}) % Scale and shift\n",
    "$$\n",
    "\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "````\n",
    "\n",
    "### Layer Normalization\n",
    "\n",
    "Layer normalization is very similar to batch normalization, but with a few key differences. Most notably, unlike batch normalization, layer normalization performs exactly the same computation at training and test times.\n",
    "\n",
    "While batch normalization computes a mean and variance for each entire mini-batch, layer normalization computes a mean and variance for each example independently.\n",
    "\n",
    "Where layer normalization differs is that it normalizes across the features (it computes the mean and variance for each example independently). In other words, each feature vector corresponding to a single datapoint is normalized based on the sum of the features.\n",
    "\n",
    "$$\n",
    "\\mu_{i} = \\frac{1}{H}\\sum_{h=1}^{H}x_{ih} % Feature mean\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma_{i}^{2} = \\frac{1}{H}\\sum_{h=1}^{H}(x_{ih} - \\mu_{i})^{2} % Feature variance\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{x}_{ih} = \\frac{x_{ih} - \\mu_{i}}{\\sqrt{\\sigma_{i}^{2} + \\varepsilon}} % Normalize\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_{ih} = \\gamma\\hat{x}_{ih} + \\beta = LN_{\\gamma,\\beta}(x_{ih}) % Scale and shift\n",
    "$$\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.layers import LayerNormalization\n",
    "model.add(LayerNormalization())\n",
    "```\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "205.8px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
